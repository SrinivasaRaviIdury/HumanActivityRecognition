{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hGngfX_QnjeK"
   },
   "outputs": [],
   "source": [
    "# !pip uninstall tensorflow\n",
    "# !pip install --upgrade tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:40:05.375061Z",
     "start_time": "2019-06-16T06:40:04.656453Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "zumILR-CxVJC"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:40:05.390686Z",
     "start_time": "2019-06-16T06:40:05.375061Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "-2uoNW3PxVJN"
   },
   "outputs": [],
   "source": [
    "# Activities are the class labels\n",
    "# It is a 6 class classification\n",
    "ACTIVITIES = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "    3: 'SITTING',\n",
    "    4: 'STANDING',\n",
    "    5: 'LAYING',\n",
    "}\n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix(Y_true, Y_pred):\n",
    "    Y_true = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([ACTIVITIES[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZbFaPG-DxVJa"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:40:06.406244Z",
     "start_time": "2019-06-16T06:40:06.390617Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ijXlVxWAxVJo"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = f'/content/drive/My Drive/LSTM_HUMAN/Human_Activity_Recognition/HAR/UCI_HAR_Dataset'\n",
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T06:40:06.671850Z",
     "start_time": "2019-06-16T06:40:06.656241Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "pKc6OLcKxVJy"
   },
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'/content/drive/My Drive/LSTM_HUMAN/Human_Activity_Recognition/HAR/UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(_read_csv(filename).values\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))\n",
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'/content/drive/My Drive/LSTM_HUMAN/Human_Activity_Recognition/HAR/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "    return pd.get_dummies(y).values\n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-16T03:11:46.856865Z",
     "start_time": "2019-06-16T03:11:46.809322Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "M4atP_OaxVLN"
   },
   "outputs": [],
   "source": [
    "# Importing tensorflow\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9yN0yU8xVLf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecfZifwwxVLn"
   },
   "outputs": [],
   "source": [
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))\n",
    "# Loading the train and test data\n",
    "X_train, X_test, Y_train, Y_test = load_data()\n",
    "timesteps = len(X_train[0])\n",
    "input_dim = len(X_train[0][0])\n",
    "n_classes = _count_classes(Y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "print(n_classes)\n",
    "print(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "NC5CRgMhxVLy",
    "outputId": "1a4632f0-1c57-44e2-9196-9884bc0487a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "6\n",
      "7352\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K2UTi1XqllGF"
   },
   "source": [
    "## Hyper parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "exhhsgj1Re6m",
    "outputId": "d968bb60-b11d-429b-f67a-d6596e15333d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7352, 128, 9)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v6_H5I8xuCx7"
   },
   "outputs": [],
   "source": [
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m_ipji3VRgKu"
   },
   "outputs": [],
   "source": [
    "def LSTM_Dropout(epochs,n_hidden,drop_rate,X_train,Y_train,X_test, Y_test,timesteps, input_dim):\n",
    "    # Initiliazing the sequential model\n",
    "    n_classes=6\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    # Adding a dropout layer\n",
    "    model.add(LSTM(n_hidden,return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "    model.add(LSTM(100,input_shape=(timesteps, input_dim)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    # Adding a dense output layer with relu activation\n",
    "    model.add(Dense(n_classes, activation='relu'))\n",
    "    model.summary()\n",
    "    # Compiling the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=\"adam\",\n",
    "                  metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=16,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=epochs)\n",
    "    print(confusion_matrix(Y_test, model.predict(X_test)))\n",
    "    score = model.evaluate(X_test, Y_test)\n",
    "    print(\"[Loss,Score]:{}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-Tm4xVD6RhB6",
    "outputId": "3232f77f-c6ab-4b99-a780-d4ba07ab587a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128, 250)          260000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               140400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 401,006\n",
      "Trainable params: 401,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /tensorflow-1.15.0/python3.6/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/50\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7352/7352 [==============================] - 213s 29ms/step - loss: 6.6714 - acc: 0.3312 - val_loss: 8.8950 - val_acc: 0.3095\n",
      "Epoch 2/50\n",
      "7352/7352 [==============================] - 203s 28ms/step - loss: 8.9704 - acc: 0.3047 - val_loss: 8.8949 - val_acc: 0.3095\n",
      "Epoch 3/50\n",
      "7352/7352 [==============================] - 207s 28ms/step - loss: 8.9679 - acc: 0.3085 - val_loss: 8.8949 - val_acc: 0.3101\n",
      "Epoch 4/50\n",
      "7352/7352 [==============================] - 206s 28ms/step - loss: 8.9745 - acc: 0.3071 - val_loss: 8.8957 - val_acc: 0.2925\n",
      "Epoch 5/50\n",
      "7352/7352 [==============================] - 205s 28ms/step - loss: 8.9657 - acc: 0.3060 - val_loss: 8.8955 - val_acc: 0.2922\n",
      "Epoch 6/50\n",
      "7352/7352 [==============================] - 208s 28ms/step - loss: 8.9656 - acc: 0.3077 - val_loss: 8.8956 - val_acc: 0.2922\n",
      "Epoch 7/50\n",
      "7352/7352 [==============================] - 208s 28ms/step - loss: 8.9834 - acc: 0.3033 - val_loss: 8.8950 - val_acc: 0.3091\n",
      "Epoch 8/50\n",
      "7352/7352 [==============================] - 203s 28ms/step - loss: 8.9832 - acc: 0.3073 - val_loss: 8.8905 - val_acc: 0.2922\n",
      "Epoch 9/50\n",
      "7352/7352 [==============================] - 204s 28ms/step - loss: 8.9679 - acc: 0.3058 - val_loss: 8.8950 - val_acc: 0.3132\n",
      "Epoch 10/50\n",
      "7352/7352 [==============================] - 204s 28ms/step - loss: 8.9592 - acc: 0.3090 - val_loss: 8.8948 - val_acc: 0.3135\n",
      "Epoch 11/50\n",
      "7352/7352 [==============================] - 205s 28ms/step - loss: 8.9548 - acc: 0.3093 - val_loss: 8.8953 - val_acc: 0.3095\n",
      "Epoch 12/50\n",
      "7352/7352 [==============================] - 205s 28ms/step - loss: 8.9572 - acc: 0.3048 - val_loss: 8.8951 - val_acc: 0.3132\n",
      "Epoch 13/50\n",
      "7352/7352 [==============================] - 207s 28ms/step - loss: 8.9569 - acc: 0.3103 - val_loss: 8.8621 - val_acc: 0.3095\n",
      "Epoch 14/50\n",
      "7352/7352 [==============================] - 203s 28ms/step - loss: 8.9330 - acc: 0.3160 - val_loss: 8.8182 - val_acc: 0.3186\n",
      "Epoch 15/50\n",
      "7352/7352 [==============================] - 208s 28ms/step - loss: 8.9308 - acc: 0.3142 - val_loss: 8.8399 - val_acc: 0.3186\n",
      "Epoch 16/50\n",
      "7352/7352 [==============================] - 207s 28ms/step - loss: 8.9329 - acc: 0.3103 - val_loss: 8.8071 - val_acc: 0.3186\n",
      "Epoch 17/50\n",
      "7352/7352 [==============================] - 205s 28ms/step - loss: 9.2425 - acc: 0.3002 - val_loss: 9.3922 - val_acc: 0.2878\n",
      "Epoch 18/50\n",
      "7352/7352 [==============================] - 205s 28ms/step - loss: 9.1106 - acc: 0.3089 - val_loss: 9.1241 - val_acc: 0.3044\n",
      "Epoch 19/50\n",
      "7352/7352 [==============================] - 205s 28ms/step - loss: 9.1081 - acc: 0.3116 - val_loss: 9.1248 - val_acc: 0.2871\n",
      "Epoch 20/50\n",
      "7352/7352 [==============================] - 202s 27ms/step - loss: 9.1084 - acc: 0.3084 - val_loss: 9.1247 - val_acc: 0.2871\n",
      "Epoch 21/50\n",
      "7352/7352 [==============================] - 203s 28ms/step - loss: 9.1082 - acc: 0.3073 - val_loss: 9.1243 - val_acc: 0.3044\n",
      "Epoch 22/50\n",
      "7352/7352 [==============================] - 204s 28ms/step - loss: 9.1084 - acc: 0.3096 - val_loss: 9.1241 - val_acc: 0.3044\n",
      "Epoch 23/50\n",
      "7352/7352 [==============================] - 204s 28ms/step - loss: 9.1083 - acc: 0.3128 - val_loss: 9.1241 - val_acc: 0.3044\n",
      "Epoch 24/50\n",
      "7352/7352 [==============================] - 207s 28ms/step - loss: 9.1082 - acc: 0.3085 - val_loss: 9.1245 - val_acc: 0.3044\n",
      "Epoch 25/50\n",
      "7352/7352 [==============================] - 204s 28ms/step - loss: 9.1082 - acc: 0.3096 - val_loss: 9.1245 - val_acc: 0.3044\n",
      "Epoch 26/50\n",
      " 720/7352 [=>............................] - ETA: 2:40 - loss: 9.0781 - acc: 0.3194"
     ]
    }
   ],
   "source": [
    "LSTM_Dropout(50,250,0.0,X_train,Y_train,X_test, Y_test,timesteps, input_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HfUilSGwvc3W"
   },
   "source": [
    "# Hidden layers and Drop outs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rzRqjuWImBzs"
   },
   "source": [
    "## More than 32 hidden layer units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KSG9rNRtxVL8"
   },
   "source": [
    "- Defining the Architecture of LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G5mB3WegxVMA"
   },
   "outputs": [],
   "source": [
    "\n",
    "def LSTM_Dropout(epochs,n_hidden,drop_rate,X_train,Y_train,X_test, Y_test,timesteps, input_dim):\n",
    "    # Initiliazing the sequential model\n",
    "    n_classes=6\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    # Adding a dropout layer\n",
    "    model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "    model.add(Dropout(drop_rate))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dense(n_classes, activation='relu',kernel_initializer=keras.initializers.glorot_normal(seed=None)))\n",
    "    model.summary()\n",
    "    # Compiling the model\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=\"rmsprop\",\n",
    "                  metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=16,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=epochs)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3aPYYlvrfddJ"
   },
   "outputs": [],
   "source": [
    "def confusion_mat(model,X_test,Y_test):\n",
    "  print(confusion_matrix(Y_test, model.predict(X_test)))\n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  print(\"[Loss,Score]:{}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "xH08n6QexVMe",
    "outputId": "3b26b125-d5e0-4c41-fbe5-0d3a08927bf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 64)                18944     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 19,334\n",
      "Trainable params: 19,334\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "7352/7352 [==============================] - 115s 16ms/step - loss: 1.6524 - acc: 0.4981 - val_loss: 1.6375 - val_acc: 0.6267\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 1.1762 - acc: 0.6891 - val_loss: 1.4328 - val_acc: 0.7201\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 0.9012 - acc: 0.7858 - val_loss: 1.2959 - val_acc: 0.8062\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 0.8744 - acc: 0.8670 - val_loss: 1.8184 - val_acc: 0.8331\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 0.8361 - acc: 0.8742 - val_loss: 1.5229 - val_acc: 0.8497\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 0.9504 - acc: 0.8881 - val_loss: 1.5600 - val_acc: 0.8782\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 105s 14ms/step - loss: 1.3347 - acc: 0.8666 - val_loss: 2.1455 - val_acc: 0.8069\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 105s 14ms/step - loss: 1.4796 - acc: 0.8309 - val_loss: 1.6696 - val_acc: 0.7167\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 1.0731 - acc: 0.8255 - val_loss: 1.2038 - val_acc: 0.8331\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 0.7403 - acc: 0.8897 - val_loss: 2.9207 - val_acc: 0.6868\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 107s 14ms/step - loss: 0.9065 - acc: 0.8555 - val_loss: 1.1089 - val_acc: 0.8812\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 106s 14ms/step - loss: 1.0334 - acc: 0.8848 - val_loss: 2.0417 - val_acc: 0.7781\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 106s 14ms/step - loss: 0.7718 - acc: 0.9070 - val_loss: 1.3360 - val_acc: 0.8666\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 1.3168 - acc: 0.8777 - val_loss: 1.5658 - val_acc: 0.8619\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 106s 14ms/step - loss: 1.3074 - acc: 0.8855 - val_loss: 2.1584 - val_acc: 0.8269\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 1.0949 - acc: 0.8901 - val_loss: 2.0563 - val_acc: 0.8415\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 1.0834 - acc: 0.8796 - val_loss: 1.1751 - val_acc: 0.8911\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 107s 15ms/step - loss: 0.8295 - acc: 0.9143 - val_loss: 1.3914 - val_acc: 0.8802\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 1.0970 - acc: 0.8949 - val_loss: 1.4239 - val_acc: 0.8656\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 108s 15ms/step - loss: 0.8980 - acc: 0.9081 - val_loss: 1.3316 - val_acc: 0.8856\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,64,0.5,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "colab_type": "code",
    "id": "DlFCFi1kkWqC",
    "outputId": "22d362c0-ae74-4d31-f233-6360f0b75fa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 510        0  ...                   0                27\n",
      "SITTING                  1      369  ...                   0                24\n",
      "STANDING                 0       65  ...                   0                 8\n",
      "WALKING                  0        0  ...                   8                59\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 406                10\n",
      "WALKING_UPSTAIRS         0        0  ...                  11               438\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 7s 2ms/step\n",
      "[Loss,Score]:[1.3316348139860328, 0.8856464200882254]\n"
     ]
    }
   ],
   "source": [
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IFWaV_zJkjej"
   },
   "source": [
    "## With 100 Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-otUC9lYiZOr",
    "outputId": "7ca927bd-c037-44df-84f1-000c7c7f2cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 100)               44000     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 606       \n",
      "=================================================================\n",
      "Total params: 44,606\n",
      "Trainable params: 44,606\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 1.2076 - acc: 0.4580 - val_loss: 1.1040 - val_acc: 0.5161\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.8808 - acc: 0.6072 - val_loss: 0.8704 - val_acc: 0.6006\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.7786 - acc: 0.6499 - val_loss: 0.7892 - val_acc: 0.6746\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.7256 - acc: 0.6870 - val_loss: 0.8060 - val_acc: 0.7000\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.6152 - acc: 0.7826 - val_loss: 0.5578 - val_acc: 0.8035\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.4348 - acc: 0.8542 - val_loss: 0.4429 - val_acc: 0.8541\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.2920 - acc: 0.9007 - val_loss: 0.4658 - val_acc: 0.8782\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.2301 - acc: 0.9241 - val_loss: 0.3556 - val_acc: 0.8860\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 44s 6ms/step - loss: 0.1990 - acc: 0.9317 - val_loss: 0.3613 - val_acc: 0.8948\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1755 - acc: 0.9370 - val_loss: 0.3518 - val_acc: 0.8996\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1747 - acc: 0.9372 - val_loss: 0.3800 - val_acc: 0.9033\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1924 - acc: 0.9361 - val_loss: 0.3735 - val_acc: 0.8789\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1726 - acc: 0.9347 - val_loss: 0.3466 - val_acc: 0.8880\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1444 - acc: 0.9465 - val_loss: 0.4098 - val_acc: 0.9019\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1459 - acc: 0.9456 - val_loss: 0.3004 - val_acc: 0.9148\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1548 - acc: 0.9434 - val_loss: 0.3664 - val_acc: 0.9050\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1413 - acc: 0.9476 - val_loss: 0.2639 - val_acc: 0.9203\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1369 - acc: 0.9498 - val_loss: 0.2749 - val_acc: 0.9097\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1388 - acc: 0.9495 - val_loss: 0.3068 - val_acc: 0.9182\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 0.1407 - acc: 0.9465 - val_loss: 0.3020 - val_acc: 0.9057\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 518        0  ...                   0                 0\n",
      "SITTING                  0      374  ...                   0                 0\n",
      "STANDING                 0       92  ...                   0                 0\n",
      "WALKING                  0        1  ...                   7                16\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 415                 3\n",
      "WALKING_UPSTAIRS         0        9  ...                   0               451\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 2s 728us/step\n",
      "[Loss,Score]:[0.30203950567234933, 0.9056667797760435]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,100,0.5,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uu8yIQlSko2k"
   },
   "source": [
    "## With 128 Layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dRioLi3UiY3j",
    "outputId": "d49a78a2-998d-4f18-b579-00171303b09b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128)               70656     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 71,430\n",
      "Trainable params: 71,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 56s 8ms/step - loss: 1.2765 - acc: 0.4480 - val_loss: 1.2189 - val_acc: 0.5049\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 1.0955 - acc: 0.5215 - val_loss: 1.3230 - val_acc: 0.3478\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.8291 - acc: 0.6215 - val_loss: 0.9279 - val_acc: 0.5389\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.8262 - acc: 0.6411 - val_loss: 0.8540 - val_acc: 0.6515\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.6176 - acc: 0.7420 - val_loss: 0.7529 - val_acc: 0.7669\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.4601 - acc: 0.8328 - val_loss: 0.5238 - val_acc: 0.8303\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.3200 - acc: 0.8932 - val_loss: 0.4218 - val_acc: 0.8839\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.2430 - acc: 0.9203 - val_loss: 0.6854 - val_acc: 0.7933\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.2530 - acc: 0.9128 - val_loss: 0.2605 - val_acc: 0.9125\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.2030 - acc: 0.9280 - val_loss: 0.3924 - val_acc: 0.8778\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.1798 - acc: 0.9339 - val_loss: 0.3233 - val_acc: 0.9074\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.1624 - acc: 0.9400 - val_loss: 0.3196 - val_acc: 0.9084\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 55s 8ms/step - loss: 0.1612 - acc: 0.9415 - val_loss: 0.2810 - val_acc: 0.9131\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.1494 - acc: 0.9427 - val_loss: 0.3219 - val_acc: 0.9118\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.1501 - acc: 0.9436 - val_loss: 0.3339 - val_acc: 0.8938\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1452 - acc: 0.9450 - val_loss: 0.2594 - val_acc: 0.9226\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1407 - acc: 0.9464 - val_loss: 0.4473 - val_acc: 0.9080\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 0.1468 - acc: 0.9484 - val_loss: 0.2846 - val_acc: 0.9121\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1350 - acc: 0.9494 - val_loss: 0.3672 - val_acc: 0.9114\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1308 - acc: 0.9497 - val_loss: 0.4315 - val_acc: 0.9121\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 513        0  ...                   0                24\n",
      "SITTING                  0      374  ...                   0                 3\n",
      "STANDING                 0       53  ...                   0                 2\n",
      "WALKING                  0        2  ...                  16                15\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 415                 3\n",
      "WALKING_UPSTAIRS         0        1  ...                  13               449\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 2s 834us/step\n",
      "[Loss,Score]:[0.4315431300159267, 0.9121140142517815]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,128,0.5,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rg2Z6zQm3lks"
   },
   "source": [
    "## 150 Hidden Layers With 0.5 Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "qLGzWt1xiYor",
    "outputId": "1823d4c2-9323-4bcb-c926-5e54f33cbfa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 150)               96000     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 6)                 906       \n",
      "=================================================================\n",
      "Total params: 96,906\n",
      "Trainable params: 96,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 66s 9ms/step - loss: 1.2182 - acc: 0.4713 - val_loss: 1.3923 - val_acc: 0.4245\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.9698 - acc: 0.5745 - val_loss: 0.9378 - val_acc: 0.5959\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.7785 - acc: 0.6668 - val_loss: 0.8223 - val_acc: 0.6688\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.6114 - acc: 0.7626 - val_loss: 0.5484 - val_acc: 0.7733\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.4228 - acc: 0.8532 - val_loss: 0.5476 - val_acc: 0.8239\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.3305 - acc: 0.8912 - val_loss: 0.5907 - val_acc: 0.8300\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2509 - acc: 0.9153 - val_loss: 0.5367 - val_acc: 0.8595\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2212 - acc: 0.9248 - val_loss: 0.5766 - val_acc: 0.8551\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1791 - acc: 0.9358 - val_loss: 0.4304 - val_acc: 0.8795\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1813 - acc: 0.9363 - val_loss: 0.4072 - val_acc: 0.8812\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1578 - acc: 0.9446 - val_loss: 0.3573 - val_acc: 0.8951\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1698 - acc: 0.9399 - val_loss: 0.4015 - val_acc: 0.8792\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1549 - acc: 0.9441 - val_loss: 0.5540 - val_acc: 0.8829\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1422 - acc: 0.9489 - val_loss: 0.5074 - val_acc: 0.8996\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.2234 - acc: 0.9268 - val_loss: 0.4011 - val_acc: 0.8962\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 0.1398 - acc: 0.9456 - val_loss: 0.3683 - val_acc: 0.9192\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1326 - acc: 0.9501 - val_loss: 0.4376 - val_acc: 0.8979\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1389 - acc: 0.9514 - val_loss: 0.5269 - val_acc: 0.9026\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1294 - acc: 0.9491 - val_loss: 0.4724 - val_acc: 0.9148\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 0.1462 - acc: 0.9479 - val_loss: 0.5625 - val_acc: 0.8985\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  0      388  ...                   0                 4\n",
      "STANDING                 0      105  ...                   0                 0\n",
      "WALKING                  0        0  ...                  55                 1\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 417                 1\n",
      "WALKING_UPSTAIRS         0        0  ...                  29               441\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 3s 1ms/step\n",
      "[Loss,Score]:[0.562234564886165, 0.8985408890397014]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,150,0.5,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ywUdQ58ixVNA"
   },
   "source": [
    "- With a simple 2 layer architecture and 64 lstm units we got 92.16% accuracy and a loss of 0.31\n",
    "- We can further imporve the performace with Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-pXEIeDrDmQ"
   },
   "source": [
    "## LSTM Training With More Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Ksq1_kvz2Qct",
    "outputId": "bbc88d3f-6cc4-4b91-bd89-e53154061ca0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 11:04:12.000723 140309728667520 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 128)               70656     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 71,430\n",
      "Trainable params: 71,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 1.2296 - acc: 0.4611 - val_loss: 1.4506 - val_acc: 0.4320\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 1.1184 - acc: 0.5131 - val_loss: 1.2153 - val_acc: 0.4781\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.9210 - acc: 0.5973 - val_loss: 0.8384 - val_acc: 0.6149\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.7467 - acc: 0.6952 - val_loss: 0.7230 - val_acc: 0.7011\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.5713 - acc: 0.7870 - val_loss: 0.5422 - val_acc: 0.7862\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.4831 - acc: 0.8293 - val_loss: 0.4780 - val_acc: 0.8337\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.3351 - acc: 0.8928 - val_loss: 0.4020 - val_acc: 0.8707\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.2611 - acc: 0.9155 - val_loss: 0.3592 - val_acc: 0.8775\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.2216 - acc: 0.9248 - val_loss: 0.3460 - val_acc: 0.8955\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1910 - acc: 0.9323 - val_loss: 0.6009 - val_acc: 0.8802\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.2071 - acc: 0.9312 - val_loss: 0.4049 - val_acc: 0.8894\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1792 - acc: 0.9399 - val_loss: 0.5514 - val_acc: 0.8870\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1876 - acc: 0.9344 - val_loss: 0.6267 - val_acc: 0.8578\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.2003 - acc: 0.9376 - val_loss: 0.4434 - val_acc: 0.8924\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 0.1883 - acc: 0.9327 - val_loss: 0.6366 - val_acc: 0.8748\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.1492 - acc: 0.9446 - val_loss: 0.6341 - val_acc: 0.8897\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.1513 - acc: 0.9459 - val_loss: 0.3936 - val_acc: 0.9036\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.1806 - acc: 0.9411 - val_loss: 0.5868 - val_acc: 0.8996\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.1519 - acc: 0.9455 - val_loss: 0.4452 - val_acc: 0.8884\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 53s 7ms/step - loss: 0.1416 - acc: 0.9475 - val_loss: 0.3308 - val_acc: 0.9101\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  0      405  ...                   0                 2\n",
      "STANDING                 0       86  ...                   0                 0\n",
      "WALKING                  0       15  ...                  29                 8\n",
      "WALKING_DOWNSTAIRS       0        2  ...                 411                 4\n",
      "WALKING_UPSTAIRS         0        5  ...                  20               441\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 2s 788us/step\n",
      "[Loss,Score]:[0.33081565343819075, 0.9100780454699695]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,128,0.6,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "RmhyataP2Umw",
    "outputId": "1d09f487-a015-44d7-f9ec-f30f2d8c8646"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0827 08:27:38.339368 139928820402048 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 128)               70656     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 71,430\n",
      "Trainable params: 71,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 1.2498 - acc: 0.4619 - val_loss: 1.3963 - val_acc: 0.4309\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.9851 - acc: 0.5596 - val_loss: 0.8483 - val_acc: 0.6121\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.8468 - acc: 0.6243 - val_loss: 0.9446 - val_acc: 0.6400\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.8163 - acc: 0.6363 - val_loss: 0.6296 - val_acc: 0.6824\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.5942 - acc: 0.7603 - val_loss: 0.7025 - val_acc: 0.7513\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.4778 - acc: 0.8331 - val_loss: 0.5366 - val_acc: 0.8507\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.3314 - acc: 0.8947 - val_loss: 0.4259 - val_acc: 0.8646\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.2584 - acc: 0.9210 - val_loss: 0.4655 - val_acc: 0.8823\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.2518 - acc: 0.9240 - val_loss: 0.4105 - val_acc: 0.8928\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.2177 - acc: 0.9283 - val_loss: 0.3603 - val_acc: 0.9030\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1964 - acc: 0.9305 - val_loss: 0.3709 - val_acc: 0.9108\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.1964 - acc: 0.9369 - val_loss: 0.3874 - val_acc: 0.8996\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 126s 17ms/step - loss: 0.1908 - acc: 0.9369 - val_loss: 0.3419 - val_acc: 0.9111\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1586 - acc: 0.9422 - val_loss: 0.3376 - val_acc: 0.9002\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 129s 18ms/step - loss: 0.1682 - acc: 0.9425 - val_loss: 0.3983 - val_acc: 0.9060\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1486 - acc: 0.9456 - val_loss: 0.4887 - val_acc: 0.9070\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1762 - acc: 0.9472 - val_loss: 0.3948 - val_acc: 0.9040\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 128s 17ms/step - loss: 0.1581 - acc: 0.9459 - val_loss: 0.5910 - val_acc: 0.9036\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 0.1685 - acc: 0.9422 - val_loss: 0.2727 - val_acc: 0.9304\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 127s 17ms/step - loss: 1.4067 - acc: 0.3501 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Pred                STANDING  WALKING\n",
      "True                                 \n",
      "LAYING                     0      537\n",
      "SITTING                    0      491\n",
      "STANDING                   0      532\n",
      "WALKING                    0      496\n",
      "WALKING_DOWNSTAIRS         0      420\n",
      "WALKING_UPSTAIRS           1      470\n",
      "2947/2947 [==============================] - 9s 3ms/step\n",
      "[Loss,Score]:[1.7911514966496784, 0.168306752629793]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,128,0.7,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "N53pzdRB2Wsw",
    "outputId": "e2dea0c2-fb59-4194-e988-3cb34e5389d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0821 12:52:54.305728 140309728667520 nn_ops.py:4224] Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_11 (LSTM)               (None, 128)               70656     \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 71,430\n",
      "Trainable params: 71,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/15\n",
      "7352/7352 [==============================] - 55s 7ms/step - loss: 1.3914 - acc: 0.3987 - val_loss: 1.2605 - val_acc: 0.4082\n",
      "Epoch 2/15\n",
      "7352/7352 [==============================] - 52s 7ms/step - loss: 1.0895 - acc: 0.5214 - val_loss: 0.9742 - val_acc: 0.5738\n",
      "Epoch 3/15\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.9079 - acc: 0.6119 - val_loss: 0.8237 - val_acc: 0.6098\n",
      "Epoch 4/15\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.8094 - acc: 0.6434 - val_loss: 0.7639 - val_acc: 0.6488\n",
      "Epoch 5/15\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.6983 - acc: 0.7031 - val_loss: 0.7344 - val_acc: 0.7452\n",
      "Epoch 6/15\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.6894 - acc: 0.7709 - val_loss: 0.5634 - val_acc: 0.8154\n",
      "Epoch 7/15\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.4651 - acc: 0.8498 - val_loss: 0.9367 - val_acc: 0.7496\n",
      "Epoch 8/15\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.3901 - acc: 0.8814 - val_loss: 0.5693 - val_acc: 0.8558\n",
      "Epoch 9/15\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3382 - acc: 0.8968 - val_loss: 0.3944 - val_acc: 0.8829\n",
      "Epoch 10/15\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.2751 - acc: 0.9166 - val_loss: 0.5295 - val_acc: 0.8819\n",
      "Epoch 11/15\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.2576 - acc: 0.9172 - val_loss: 0.7339 - val_acc: 0.7642\n",
      "Epoch 12/15\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.2275 - acc: 0.9264 - val_loss: 0.6357 - val_acc: 0.8694\n",
      "Epoch 13/15\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.2178 - acc: 0.9355 - val_loss: 0.5925 - val_acc: 0.8965\n",
      "Epoch 14/15\n",
      "7352/7352 [==============================] - 48s 7ms/step - loss: 0.2170 - acc: 0.9370 - val_loss: 0.5521 - val_acc: 0.8856\n",
      "Epoch 15/15\n",
      "7352/7352 [==============================] - 48s 7ms/step - loss: 0.2544 - acc: 0.9287 - val_loss: 0.5666 - val_acc: 0.8887\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 508        0  ...                   0                26\n",
      "SITTING                  2      374  ...                   0                 3\n",
      "STANDING                 0       97  ...                   0                 0\n",
      "WALKING                  0        0  ...                  32                 4\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 416                 0\n",
      "WALKING_UPSTAIRS         0        0  ...                   7               428\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 2s 712us/step\n",
      "[Loss,Score]:[0.5667599796087442, 0.8887003732609433]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(15,128,0.8,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dKCG158J2YwA",
    "outputId": "c509e673-158c-4972-887c-54fceaa66a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 128)               70656     \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 71,430\n",
      "Trainable params: 71,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 54s 7ms/step - loss: 1.2076 - acc: 0.4694 - val_loss: 1.4387 - val_acc: 0.3848\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.9906 - acc: 0.5541 - val_loss: 0.9297 - val_acc: 0.6732\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.7084 - acc: 0.7043 - val_loss: 0.7242 - val_acc: 0.6875\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.5887 - acc: 0.7816 - val_loss: 0.6235 - val_acc: 0.7794\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.4371 - acc: 0.8428 - val_loss: 0.8353 - val_acc: 0.7581\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.3925 - acc: 0.8724 - val_loss: 0.4806 - val_acc: 0.8280\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.2781 - acc: 0.9044 - val_loss: 0.5144 - val_acc: 0.8402\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.2226 - acc: 0.9197 - val_loss: 0.3450 - val_acc: 0.8833\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1839 - acc: 0.9339 - val_loss: 0.2499 - val_acc: 0.9019\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1763 - acc: 0.9400 - val_loss: 0.5409 - val_acc: 0.8789\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1646 - acc: 0.9366 - val_loss: 0.3462 - val_acc: 0.8992\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1482 - acc: 0.9468 - val_loss: 0.4704 - val_acc: 0.8850\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1487 - acc: 0.9444 - val_loss: 0.2670 - val_acc: 0.9138\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.1513 - acc: 0.9436 - val_loss: 0.5030 - val_acc: 0.8935\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.1368 - acc: 0.9463 - val_loss: 0.3148 - val_acc: 0.9175\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.1288 - acc: 0.9508 - val_loss: 0.3866 - val_acc: 0.9077\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 49s 7ms/step - loss: 0.1381 - acc: 0.9445 - val_loss: 0.3889 - val_acc: 0.9009\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1305 - acc: 0.9440 - val_loss: 0.7021 - val_acc: 0.8772\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1330 - acc: 0.9460 - val_loss: 0.5393 - val_acc: 0.8985\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.1343 - acc: 0.9459 - val_loss: 0.5020 - val_acc: 0.8870\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  6      391  ...                   0                 1\n",
      "STANDING                 0      118  ...                   0                 0\n",
      "WALKING                  0        0  ...                  30                 8\n",
      "WALKING_DOWNSTAIRS       0        3  ...                 409                 4\n",
      "WALKING_UPSTAIRS         0        0  ...                  23               406\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 2s 733us/step\n",
      "[Loss,Score]:[0.5019753047827739, 0.8870037326094333]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,128,0.4,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "# Confusion Matrix\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5MG7KhQ8knhr"
   },
   "source": [
    "- With a simple one LSTM layer architecture and 64 lstm units we got 86.35% accuracy and a loss of 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfAyY7ycEoHA"
   },
   "source": [
    "## Two Layer LSTM Without Dropout 128 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C1M9ObkxsZrx"
   },
   "outputs": [],
   "source": [
    "def LSTM_Dropout(epochs,n_hidden,X_train,Y_train,X_test, Y_test,timesteps, input_dim):\n",
    "    # Initiliazing the sequential model\n",
    "    n_classes=6\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    # Adding a dropout layer\n",
    "    model.add(LSTM(n_hidden, return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "    model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dense(n_classes, activation='sigmoid',kernel_initializer=keras.initializers.glorot_normal(seed=None),bias_initializer=keras.initializers.Ones()))\n",
    "    model.summary()\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=16,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def confusion_mat(model,X_test,Y_test):\n",
    "  print(confusion_matrix(Y_test, model.predict(X_test)))\n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  print(\"[Loss,Score]:{}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UadZU585b2HT",
    "outputId": "7afd3909-0fb8-48c4-a9bd-cbb93cbe2af9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_13 (LSTM)               (None, 128, 128)          70656     \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 203,014\n",
      "Trainable params: 203,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 129s 17ms/step - loss: 0.9495 - acc: 0.5842 - val_loss: 0.7855 - val_acc: 0.6590\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.5942 - acc: 0.7601 - val_loss: 0.7787 - val_acc: 0.7197\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.3552 - acc: 0.8754 - val_loss: 0.5013 - val_acc: 0.8660\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.2030 - acc: 0.9249 - val_loss: 0.2593 - val_acc: 0.8996\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.1708 - acc: 0.9384 - val_loss: 0.3663 - val_acc: 0.8748\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 124s 17ms/step - loss: 0.1504 - acc: 0.9423 - val_loss: 0.3398 - val_acc: 0.8982\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.1487 - acc: 0.9455 - val_loss: 0.3130 - val_acc: 0.9057\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.1397 - acc: 0.9456 - val_loss: 0.2771 - val_acc: 0.9121\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 122s 17ms/step - loss: 0.1432 - acc: 0.9450 - val_loss: 0.3550 - val_acc: 0.8979\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1317 - acc: 0.9479 - val_loss: 0.3857 - val_acc: 0.9148\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 121s 16ms/step - loss: 0.1278 - acc: 0.9509 - val_loss: 0.2080 - val_acc: 0.9165\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1298 - acc: 0.9521 - val_loss: 0.2851 - val_acc: 0.9104\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.1387 - acc: 0.9484 - val_loss: 0.4185 - val_acc: 0.9128\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 123s 17ms/step - loss: 0.1281 - acc: 0.9513 - val_loss: 0.4777 - val_acc: 0.8951\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1191 - acc: 0.9521 - val_loss: 0.4574 - val_acc: 0.8907\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1246 - acc: 0.9516 - val_loss: 0.2837 - val_acc: 0.9036\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1164 - acc: 0.9536 - val_loss: 0.4578 - val_acc: 0.8968\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1237 - acc: 0.9502 - val_loss: 0.3535 - val_acc: 0.9114\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 119s 16ms/step - loss: 0.1296 - acc: 0.9516 - val_loss: 0.3685 - val_acc: 0.9097\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 120s 16ms/step - loss: 0.1181 - acc: 0.9524 - val_loss: 0.3255 - val_acc: 0.9270\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 536        0  ...                   0                 1\n",
      "SITTING                  5      413  ...                   0                 2\n",
      "STANDING                 0       77  ...                   0                 0\n",
      "WALKING                  0        0  ...                  31                10\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 418                 1\n",
      "WALKING_UPSTAIRS         0        0  ...                  15               455\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 5s 2ms/step\n",
      "[Loss,Score]:[0.32553092682233864, 0.9270444519850696]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,128,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z1OMRw83lSk6"
   },
   "source": [
    "## Two Layer LSTM With Dropout(0.7) and 128 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pDqZvHvNcc-C"
   },
   "outputs": [],
   "source": [
    "def LSTM_Dropout(epochs,n_hidden,Drop,X_train,Y_train,X_test, Y_test,timesteps, input_dim):\n",
    "    # Initiliazing the sequential model\n",
    "    n_classes=6\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    # Adding a dropout layer\n",
    "    model.add(LSTM(n_hidden, return_sequences=True, input_shape=(timesteps, input_dim)))\n",
    "    model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "    model.add(Dropout(Drop))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dense(n_classes, activation='sigmoid',kernel_initializer=keras.initializers.glorot_normal(seed=None),bias_initializer=keras.initializers.Ones()))\n",
    "    model.summary()\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=50,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def confusion_mat(model,X_test,Y_test):\n",
    "  print(confusion_matrix(Y_test, model.predict(X_test)))\n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  print(\"[Loss,Score]:{}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2GYmTuvUhrFD",
    "outputId": "7f1a712d-863e-48e4-b14b-b7304cdeb385"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (None, 128, 128)          70656     \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 203,014\n",
      "Trainable params: 203,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 1.1817 - acc: 0.4758 - val_loss: 1.0626 - val_acc: 0.5137\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.8075 - acc: 0.6240 - val_loss: 0.9101 - val_acc: 0.6060\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.7268 - acc: 0.6585 - val_loss: 0.8987 - val_acc: 0.6318\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.7358 - acc: 0.6532 - val_loss: 0.8478 - val_acc: 0.6094\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.6752 - acc: 0.6927 - val_loss: 0.9961 - val_acc: 0.4934\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.5184 - acc: 0.7952 - val_loss: 0.7065 - val_acc: 0.7241\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.3831 - acc: 0.8576 - val_loss: 0.8833 - val_acc: 0.6980\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.2973 - acc: 0.8950 - val_loss: 0.3471 - val_acc: 0.8714\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.2249 - acc: 0.9170 - val_loss: 0.2841 - val_acc: 0.9036\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1770 - acc: 0.9393 - val_loss: 0.4822 - val_acc: 0.8734\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1732 - acc: 0.9397 - val_loss: 0.3521 - val_acc: 0.8826\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1521 - acc: 0.9436 - val_loss: 0.3987 - val_acc: 0.8921\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1406 - acc: 0.9467 - val_loss: 0.2813 - val_acc: 0.9094\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1436 - acc: 0.9465 - val_loss: 0.2712 - val_acc: 0.9080\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1249 - acc: 0.9505 - val_loss: 0.2179 - val_acc: 0.9220\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1253 - acc: 0.9476 - val_loss: 0.2862 - val_acc: 0.9108\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1210 - acc: 0.9532 - val_loss: 0.3599 - val_acc: 0.8918\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1327 - acc: 0.9479 - val_loss: 0.3269 - val_acc: 0.9128\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1161 - acc: 0.9525 - val_loss: 0.2180 - val_acc: 0.9233\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1208 - acc: 0.9505 - val_loss: 0.4356 - val_acc: 0.8938\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 510        0  ...                   0                 0\n",
      "SITTING                  0      288  ...                   0                 0\n",
      "STANDING                 0       34  ...                   0                 0\n",
      "WALKING                  0        0  ...                  33                 4\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 420                 0\n",
      "WALKING_UPSTAIRS         0        0  ...                  12               459\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 18s 6ms/step\n",
      "[Loss,Score]:[0.4355675253932894, 0.8937902952154734]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,128,0,X_train,Y_train,X_test, Y_test,128, 9)\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "dZFIIw0f-w9z",
    "outputId": "40a86f69-1487-428e-9dca-cb8ae4b5334b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_19 (LSTM)               (None, 128, 128)          70656     \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 203,014\n",
      "Trainable params: 203,014\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 76s 10ms/step - loss: 1.1536 - acc: 0.5120 - val_loss: 0.9958 - val_acc: 0.5606\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.8785 - acc: 0.6020 - val_loss: 1.1747 - val_acc: 0.4920\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.7567 - acc: 0.6411 - val_loss: 0.7209 - val_acc: 0.6434\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.7976 - acc: 0.6243 - val_loss: 0.7701 - val_acc: 0.6451\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.6862 - acc: 0.6680 - val_loss: 0.8498 - val_acc: 0.6393\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.6581 - acc: 0.6993 - val_loss: 0.7689 - val_acc: 0.6759\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.6949 - acc: 0.6805 - val_loss: 0.7966 - val_acc: 0.6770\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.6015 - acc: 0.7402 - val_loss: 0.6844 - val_acc: 0.7469\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.5999 - acc: 0.7552 - val_loss: 0.6485 - val_acc: 0.8032\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.6003 - acc: 0.7930 - val_loss: 0.7456 - val_acc: 0.7218\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.4385 - acc: 0.8498 - val_loss: 0.7008 - val_acc: 0.8059\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.2997 - acc: 0.9115 - val_loss: 0.5147 - val_acc: 0.8670\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.2958 - acc: 0.9151 - val_loss: 0.4623 - val_acc: 0.8901\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 68s 9ms/step - loss: 0.2413 - acc: 0.9244 - val_loss: 0.4395 - val_acc: 0.8962\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.2388 - acc: 0.9240 - val_loss: 0.5260 - val_acc: 0.8690\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.2246 - acc: 0.9316 - val_loss: 0.4333 - val_acc: 0.8931\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.2323 - acc: 0.9314 - val_loss: 0.6547 - val_acc: 0.8588\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.2170 - acc: 0.9290 - val_loss: 0.3731 - val_acc: 0.8921\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1928 - acc: 0.9396 - val_loss: 0.3605 - val_acc: 0.9019\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1657 - acc: 0.9411 - val_loss: 0.5411 - val_acc: 0.8860\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 510        0  ...                   0                 0\n",
      "SITTING                  0      401  ...                   0                 2\n",
      "STANDING                 0      101  ...                   0                 0\n",
      "WALKING                  0        1  ...                  27                 0\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 408                 0\n",
      "WALKING_UPSTAIRS         0        7  ...                  18               396\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 6s 2ms/step\n",
      "[Loss,Score]:[0.5410958211561075, 0.8859857482185273]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(20,128,0.8,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LTtpbBxDuJmh"
   },
   "outputs": [],
   "source": [
    "def LSTM_Dropout(epochs,n_hidden,D,batch_S,X_train,Y_train,X_test, Y_test,timesteps, input_dim):\n",
    "    # Initiliazing the sequential model\n",
    "    n_classes=6\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    # Adding a dropout layer\n",
    "    model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "    model.add(Dropout(D))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dense(n_classes, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=batch_S,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def confusion_mat(model,X_test,Y_test):\n",
    "  print(confusion_matrix(Y_test, model.predict(X_test)))\n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  print(\"[Loss,Score]:{}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IPrSXyaSvikk",
    "outputId": "97b6b3d2-773b-420c-f1c7-2fc7323ffda0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_12 (LSTM)               (None, 1000)              4040000   \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 6)                 6006      \n",
      "=================================================================\n",
      "Total params: 4,046,006\n",
      "Trainable params: 4,046,006\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/100\n",
      "7352/7352 [==============================] - 16s 2ms/step - loss: 1.4028 - acc: 0.3372 - val_loss: 1.4209 - val_acc: 0.3359\n",
      "Epoch 2/100\n",
      "7352/7352 [==============================] - 14s 2ms/step - loss: 1.3689 - acc: 0.3591 - val_loss: 1.4061 - val_acc: 0.3482\n",
      "Epoch 3/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3653 - acc: 0.3705 - val_loss: 1.4191 - val_acc: 0.3451\n",
      "Epoch 4/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3696 - acc: 0.3682 - val_loss: 1.3918 - val_acc: 0.3468\n",
      "Epoch 5/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3701 - acc: 0.3645 - val_loss: 1.3784 - val_acc: 0.3482\n",
      "Epoch 6/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3520 - acc: 0.3683 - val_loss: 1.3643 - val_acc: 0.3482\n",
      "Epoch 7/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.4447 - acc: 0.3507 - val_loss: 1.7827 - val_acc: 0.1666\n",
      "Epoch 8/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.7418 - acc: 0.2203 - val_loss: 2.0031 - val_acc: 0.1666\n",
      "Epoch 9/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.6580 - acc: 0.3112 - val_loss: 1.3423 - val_acc: 0.4058\n",
      "Epoch 10/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.4260 - acc: 0.3875 - val_loss: 1.3186 - val_acc: 0.4391\n",
      "Epoch 11/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.2787 - acc: 0.4445 - val_loss: 1.5045 - val_acc: 0.3651\n",
      "Epoch 12/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3948 - acc: 0.3973 - val_loss: 1.3263 - val_acc: 0.3994\n",
      "Epoch 13/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.2660 - acc: 0.4415 - val_loss: 1.2709 - val_acc: 0.4364\n",
      "Epoch 14/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.2261 - acc: 0.4559 - val_loss: 1.2178 - val_acc: 0.4415\n",
      "Epoch 15/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.1863 - acc: 0.4773 - val_loss: 1.2444 - val_acc: 0.4571\n",
      "Epoch 16/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.1604 - acc: 0.4627 - val_loss: 1.2578 - val_acc: 0.4014\n",
      "Epoch 17/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.1619 - acc: 0.4521 - val_loss: 1.8169 - val_acc: 0.3271\n",
      "Epoch 18/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3489 - acc: 0.3441 - val_loss: 1.4736 - val_acc: 0.3281\n",
      "Epoch 19/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3282 - acc: 0.3371 - val_loss: 1.3993 - val_acc: 0.3278\n",
      "Epoch 20/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3425 - acc: 0.3860 - val_loss: 1.1398 - val_acc: 0.4666\n",
      "Epoch 21/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.1882 - acc: 0.4456 - val_loss: 1.2282 - val_acc: 0.4282\n",
      "Epoch 22/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.2129 - acc: 0.4183 - val_loss: 1.3500 - val_acc: 0.4489\n",
      "Epoch 23/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3228 - acc: 0.3740 - val_loss: 1.4298 - val_acc: 0.3186\n",
      "Epoch 24/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3735 - acc: 0.3490 - val_loss: 1.4375 - val_acc: 0.3342\n",
      "Epoch 25/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.4203 - acc: 0.3365 - val_loss: 1.4327 - val_acc: 0.3149\n",
      "Epoch 26/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.4787 - acc: 0.3260 - val_loss: 1.5182 - val_acc: 0.2973\n",
      "Epoch 27/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3679 - acc: 0.3706 - val_loss: 1.3554 - val_acc: 0.3627\n",
      "Epoch 28/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3056 - acc: 0.3856 - val_loss: 1.4604 - val_acc: 0.3366\n",
      "Epoch 29/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.2885 - acc: 0.3795 - val_loss: 1.4093 - val_acc: 0.3448\n",
      "Epoch 30/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3197 - acc: 0.3689 - val_loss: 1.4180 - val_acc: 0.3400\n",
      "Epoch 31/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3271 - acc: 0.3580 - val_loss: 1.4519 - val_acc: 0.3363\n",
      "Epoch 32/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3199 - acc: 0.3583 - val_loss: 1.4049 - val_acc: 0.3359\n",
      "Epoch 33/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3087 - acc: 0.3747 - val_loss: 1.6035 - val_acc: 0.3291\n",
      "Epoch 34/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.2822 - acc: 0.3769 - val_loss: 1.7086 - val_acc: 0.2097\n",
      "Epoch 35/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3379 - acc: 0.3557 - val_loss: 1.3302 - val_acc: 0.3468\n",
      "Epoch 36/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3907 - acc: 0.3364 - val_loss: 1.3215 - val_acc: 0.3502\n",
      "Epoch 37/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.3136 - acc: 0.4090 - val_loss: 1.2340 - val_acc: 0.4506\n",
      "Epoch 38/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 1.0301 - acc: 0.5605 - val_loss: 0.8075 - val_acc: 0.6963\n",
      "Epoch 39/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.9383 - acc: 0.6140 - val_loss: 0.7492 - val_acc: 0.7245\n",
      "Epoch 40/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.8216 - acc: 0.6504 - val_loss: 0.7333 - val_acc: 0.6776\n",
      "Epoch 41/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.7105 - acc: 0.6922 - val_loss: 0.6578 - val_acc: 0.7367\n",
      "Epoch 42/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.6467 - acc: 0.7320 - val_loss: 0.5220 - val_acc: 0.7832\n",
      "Epoch 43/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.6192 - acc: 0.7255 - val_loss: 0.5879 - val_acc: 0.7581\n",
      "Epoch 44/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.5082 - acc: 0.8025 - val_loss: 0.5628 - val_acc: 0.7808\n",
      "Epoch 45/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.4377 - acc: 0.8278 - val_loss: 0.6533 - val_acc: 0.7333\n",
      "Epoch 46/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.3496 - acc: 0.8672 - val_loss: 0.3699 - val_acc: 0.8375\n",
      "Epoch 47/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.3303 - acc: 0.8867 - val_loss: 0.3437 - val_acc: 0.8656\n",
      "Epoch 48/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.3020 - acc: 0.8868 - val_loss: 0.2775 - val_acc: 0.8867\n",
      "Epoch 49/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.3165 - acc: 0.8862 - val_loss: 0.5245 - val_acc: 0.7886\n",
      "Epoch 50/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.2330 - acc: 0.9076 - val_loss: 0.3631 - val_acc: 0.8554\n",
      "Epoch 51/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.2140 - acc: 0.9154 - val_loss: 0.3233 - val_acc: 0.8918\n",
      "Epoch 52/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.2441 - acc: 0.9027 - val_loss: 0.2595 - val_acc: 0.8914\n",
      "Epoch 53/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1819 - acc: 0.9246 - val_loss: 0.6898 - val_acc: 0.7489\n",
      "Epoch 54/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.2077 - acc: 0.9217 - val_loss: 0.3127 - val_acc: 0.8921\n",
      "Epoch 55/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1871 - acc: 0.9308 - val_loss: 0.3104 - val_acc: 0.8812\n",
      "Epoch 56/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.2111 - acc: 0.9256 - val_loss: 0.3117 - val_acc: 0.8846\n",
      "Epoch 57/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1705 - acc: 0.9350 - val_loss: 0.2695 - val_acc: 0.8924\n",
      "Epoch 58/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1609 - acc: 0.9346 - val_loss: 0.2934 - val_acc: 0.9023\n",
      "Epoch 59/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.2347 - acc: 0.9278 - val_loss: 0.3692 - val_acc: 0.8924\n",
      "Epoch 60/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1942 - acc: 0.9267 - val_loss: 0.3304 - val_acc: 0.9040\n",
      "Epoch 61/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1622 - acc: 0.9368 - val_loss: 0.2946 - val_acc: 0.8992\n",
      "Epoch 62/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1654 - acc: 0.9403 - val_loss: 0.2876 - val_acc: 0.9050\n",
      "Epoch 63/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1309 - acc: 0.9463 - val_loss: 0.3473 - val_acc: 0.9002\n",
      "Epoch 64/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1716 - acc: 0.9368 - val_loss: 0.8050 - val_acc: 0.8012\n",
      "Epoch 65/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1290 - acc: 0.9450 - val_loss: 0.3222 - val_acc: 0.9023\n",
      "Epoch 66/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.2033 - acc: 0.9314 - val_loss: 0.2814 - val_acc: 0.9057\n",
      "Epoch 67/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1494 - acc: 0.9391 - val_loss: 0.2848 - val_acc: 0.8982\n",
      "Epoch 68/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1408 - acc: 0.9400 - val_loss: 0.2795 - val_acc: 0.9084\n",
      "Epoch 69/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1277 - acc: 0.9460 - val_loss: 0.4079 - val_acc: 0.8897\n",
      "Epoch 70/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1170 - acc: 0.9514 - val_loss: 0.4419 - val_acc: 0.8928\n",
      "Epoch 71/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1518 - acc: 0.9436 - val_loss: 0.3747 - val_acc: 0.8979\n",
      "Epoch 72/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1191 - acc: 0.9514 - val_loss: 0.3104 - val_acc: 0.9063\n",
      "Epoch 73/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1111 - acc: 0.9486 - val_loss: 0.4327 - val_acc: 0.9019\n",
      "Epoch 74/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1299 - acc: 0.9499 - val_loss: 0.3738 - val_acc: 0.9026\n",
      "Epoch 75/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1526 - acc: 0.9452 - val_loss: 0.3208 - val_acc: 0.9152\n",
      "Epoch 76/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1193 - acc: 0.9509 - val_loss: 0.3638 - val_acc: 0.9040\n",
      "Epoch 77/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1144 - acc: 0.9493 - val_loss: 0.4418 - val_acc: 0.8965\n",
      "Epoch 78/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1578 - acc: 0.9406 - val_loss: 0.2628 - val_acc: 0.9203\n",
      "Epoch 79/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1099 - acc: 0.9508 - val_loss: 0.3843 - val_acc: 0.8996\n",
      "Epoch 80/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1294 - acc: 0.9486 - val_loss: 0.3197 - val_acc: 0.8921\n",
      "Epoch 81/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1119 - acc: 0.9513 - val_loss: 0.3320 - val_acc: 0.9118\n",
      "Epoch 82/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1175 - acc: 0.9497 - val_loss: 0.3572 - val_acc: 0.8965\n",
      "Epoch 83/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1185 - acc: 0.9506 - val_loss: 0.4987 - val_acc: 0.8728\n",
      "Epoch 84/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1177 - acc: 0.9532 - val_loss: 0.4738 - val_acc: 0.8853\n",
      "Epoch 85/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1046 - acc: 0.9553 - val_loss: 0.4230 - val_acc: 0.9013\n",
      "Epoch 86/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1439 - acc: 0.9402 - val_loss: 0.4166 - val_acc: 0.8938\n",
      "Epoch 87/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.0965 - acc: 0.9567 - val_loss: 0.5281 - val_acc: 0.8931\n",
      "Epoch 88/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1258 - acc: 0.9445 - val_loss: 0.3740 - val_acc: 0.8829\n",
      "Epoch 89/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1261 - acc: 0.9514 - val_loss: 0.3355 - val_acc: 0.9114\n",
      "Epoch 90/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1225 - acc: 0.9482 - val_loss: 0.3355 - val_acc: 0.9026\n",
      "Epoch 91/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1313 - acc: 0.9490 - val_loss: 0.3613 - val_acc: 0.8887\n",
      "Epoch 92/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.0905 - acc: 0.9570 - val_loss: 0.5164 - val_acc: 0.8802\n",
      "Epoch 93/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1325 - acc: 0.9440 - val_loss: 0.4098 - val_acc: 0.8778\n",
      "Epoch 94/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1024 - acc: 0.9544 - val_loss: 0.4969 - val_acc: 0.8897\n",
      "Epoch 95/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1215 - acc: 0.9502 - val_loss: 0.5711 - val_acc: 0.8918\n",
      "Epoch 96/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1067 - acc: 0.9532 - val_loss: 0.4697 - val_acc: 0.8880\n",
      "Epoch 97/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1014 - acc: 0.9558 - val_loss: 0.4774 - val_acc: 0.8833\n",
      "Epoch 98/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1068 - acc: 0.9528 - val_loss: 1.7681 - val_acc: 0.7581\n",
      "Epoch 99/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1394 - acc: 0.9504 - val_loss: 0.4443 - val_acc: 0.9023\n",
      "Epoch 100/100\n",
      "7352/7352 [==============================] - 13s 2ms/step - loss: 0.1199 - acc: 0.9529 - val_loss: 0.4358 - val_acc: 0.8911\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 517        0  ...                   0                20\n",
      "SITTING                  0      370  ...                   0                 1\n",
      "STANDING                 0       71  ...                   0                 0\n",
      "WALKING                  0        2  ...                  26                36\n",
      "WALKING_DOWNSTAIRS       1        2  ...                 400                15\n",
      "WALKING_UPSTAIRS         0        0  ...                   9               446\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 6s 2ms/step\n",
      "[Loss,Score]:[0.4361309705248378, 0.8910756701730573]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout(100,1000,0.5,200,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wQ2MpS4trs71"
   },
   "outputs": [],
   "source": [
    "def LSTM_Dropout_Grid(epochs,n_hidden,D,batch_S,X_train,Y_train,X_test, Y_test,timesteps, input_dim):\n",
    "    # Initiliazing the sequential model\n",
    "    n_classes=6\n",
    "    model = Sequential()\n",
    "    # Configuring the parameters\n",
    "    # Adding a dropout layer\n",
    "    model.add(LSTM(n_hidden, input_shape=(timesteps, input_dim)))\n",
    "    model.add(Dropout(D))\n",
    "    # Adding a dense output layer with sigmoid activation\n",
    "    model.add(Dense(n_classes, activation='sigmoid'))\n",
    "    model.summary()\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=batch_S,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def confusion_mat(model,X_test,Y_test):\n",
    "  print(confusion_matrix(Y_test, model.predict(X_test)))\n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  print(\"[Loss,Score]:{}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "Fs-bz1RsLOnd",
    "outputId": "ba3cdeeb-10e5-4695-a06b-f7b8e7983a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, 150)               96000     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 906       \n",
      "=================================================================\n",
      "Total params: 96,906\n",
      "Trainable params: 96,906\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/50\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 1.4270 - acc: 0.3685 - val_loss: 1.3563 - val_acc: 0.3536\n",
      "Epoch 2/50\n",
      "7352/7352 [==============================] - 7s 987us/step - loss: 1.2933 - acc: 0.4180 - val_loss: 1.4604 - val_acc: 0.3393\n",
      "Epoch 3/50\n",
      "7352/7352 [==============================] - 7s 975us/step - loss: 1.2651 - acc: 0.4395 - val_loss: 1.3616 - val_acc: 0.4265\n",
      "Epoch 4/50\n",
      "7352/7352 [==============================] - 7s 981us/step - loss: 1.2588 - acc: 0.4399 - val_loss: 1.2466 - val_acc: 0.4771\n",
      "Epoch 5/50\n",
      "7352/7352 [==============================] - 7s 993us/step - loss: 1.0476 - acc: 0.5122 - val_loss: 1.0393 - val_acc: 0.5823\n",
      "Epoch 6/50\n",
      "7352/7352 [==============================] - 7s 996us/step - loss: 1.2883 - acc: 0.4422 - val_loss: 1.4356 - val_acc: 0.2810\n",
      "Epoch 7/50\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 1.0872 - acc: 0.4791 - val_loss: 0.9379 - val_acc: 0.4778\n",
      "Epoch 8/50\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.8917 - acc: 0.5322 - val_loss: 0.8842 - val_acc: 0.5687\n",
      "Epoch 9/50\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.8484 - acc: 0.5781 - val_loss: 0.8060 - val_acc: 0.6006\n",
      "Epoch 10/50\n",
      "7352/7352 [==============================] - 7s 990us/step - loss: 0.8058 - acc: 0.5997 - val_loss: 0.9289 - val_acc: 0.5752\n",
      "Epoch 11/50\n",
      "7352/7352 [==============================] - 7s 995us/step - loss: 0.8542 - acc: 0.5953 - val_loss: 0.8608 - val_acc: 0.5931\n",
      "Epoch 12/50\n",
      "7352/7352 [==============================] - 7s 989us/step - loss: 0.7367 - acc: 0.6453 - val_loss: 0.7849 - val_acc: 0.6332\n",
      "Epoch 13/50\n",
      "7352/7352 [==============================] - 7s 996us/step - loss: 0.7245 - acc: 0.6654 - val_loss: 0.6811 - val_acc: 0.6844\n",
      "Epoch 14/50\n",
      "7352/7352 [==============================] - 7s 998us/step - loss: 0.7096 - acc: 0.6541 - val_loss: 0.8210 - val_acc: 0.6145\n",
      "Epoch 15/50\n",
      "7352/7352 [==============================] - 7s 992us/step - loss: 0.6974 - acc: 0.6729 - val_loss: 0.7184 - val_acc: 0.6451\n",
      "Epoch 16/50\n",
      "7352/7352 [==============================] - 7s 998us/step - loss: 0.6232 - acc: 0.7070 - val_loss: 0.6812 - val_acc: 0.6420\n",
      "Epoch 17/50\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.5953 - acc: 0.7325 - val_loss: 0.5920 - val_acc: 0.6997\n",
      "Epoch 18/50\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.5324 - acc: 0.7719 - val_loss: 0.5993 - val_acc: 0.7072\n",
      "Epoch 19/50\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.5008 - acc: 0.7899 - val_loss: 0.6001 - val_acc: 0.7465\n",
      "Epoch 20/50\n",
      "7352/7352 [==============================] - 7s 979us/step - loss: 0.5026 - acc: 0.8112 - val_loss: 0.5613 - val_acc: 0.7716\n",
      "Epoch 21/50\n",
      "7352/7352 [==============================] - 7s 968us/step - loss: 0.4562 - acc: 0.8266 - val_loss: 0.5206 - val_acc: 0.8117\n",
      "Epoch 22/50\n",
      "7352/7352 [==============================] - 7s 980us/step - loss: 0.3977 - acc: 0.8629 - val_loss: 0.4419 - val_acc: 0.8476\n",
      "Epoch 23/50\n",
      "7352/7352 [==============================] - 7s 985us/step - loss: 0.4121 - acc: 0.8659 - val_loss: 0.5695 - val_acc: 0.8259\n",
      "Epoch 24/50\n",
      "7352/7352 [==============================] - 7s 983us/step - loss: 0.3498 - acc: 0.8761 - val_loss: 0.4291 - val_acc: 0.8592\n",
      "Epoch 25/50\n",
      "7352/7352 [==============================] - 7s 993us/step - loss: 0.3110 - acc: 0.8945 - val_loss: 0.6724 - val_acc: 0.7944\n",
      "Epoch 26/50\n",
      "7352/7352 [==============================] - 7s 978us/step - loss: 0.2557 - acc: 0.9165 - val_loss: 0.4274 - val_acc: 0.8534\n",
      "Epoch 27/50\n",
      "7352/7352 [==============================] - 7s 969us/step - loss: 0.2442 - acc: 0.9162 - val_loss: 0.2762 - val_acc: 0.8985\n",
      "Epoch 28/50\n",
      "7352/7352 [==============================] - 7s 983us/step - loss: 0.2262 - acc: 0.9255 - val_loss: 0.2711 - val_acc: 0.9108\n",
      "Epoch 29/50\n",
      "7352/7352 [==============================] - 7s 987us/step - loss: 0.2117 - acc: 0.9248 - val_loss: 0.2599 - val_acc: 0.9063\n",
      "Epoch 30/50\n",
      "7352/7352 [==============================] - 8s 1ms/step - loss: 0.2226 - acc: 0.9282 - val_loss: 0.2902 - val_acc: 0.9057\n",
      "Epoch 31/50\n",
      "7352/7352 [==============================] - 7s 985us/step - loss: 0.1792 - acc: 0.9340 - val_loss: 0.2658 - val_acc: 0.9094\n",
      "Epoch 32/50\n",
      "7352/7352 [==============================] - 7s 980us/step - loss: 0.2136 - acc: 0.9287 - val_loss: 0.4027 - val_acc: 0.8588\n",
      "Epoch 33/50\n",
      "7352/7352 [==============================] - 7s 990us/step - loss: 0.2090 - acc: 0.9253 - val_loss: 0.2547 - val_acc: 0.9091\n",
      "Epoch 34/50\n",
      "7352/7352 [==============================] - 7s 979us/step - loss: 0.1606 - acc: 0.9368 - val_loss: 0.2330 - val_acc: 0.9131\n",
      "Epoch 35/50\n",
      "7352/7352 [==============================] - 7s 987us/step - loss: 0.2035 - acc: 0.9331 - val_loss: 0.2570 - val_acc: 0.9108\n",
      "Epoch 36/50\n",
      "7352/7352 [==============================] - 7s 988us/step - loss: 0.1657 - acc: 0.9429 - val_loss: 0.2693 - val_acc: 0.9057\n",
      "Epoch 37/50\n",
      "7352/7352 [==============================] - 7s 987us/step - loss: 0.1984 - acc: 0.9365 - val_loss: 0.2010 - val_acc: 0.9250\n",
      "Epoch 38/50\n",
      "7352/7352 [==============================] - 7s 983us/step - loss: 0.1738 - acc: 0.9369 - val_loss: 0.2015 - val_acc: 0.9233\n",
      "Epoch 39/50\n",
      "7352/7352 [==============================] - 7s 986us/step - loss: 0.1471 - acc: 0.9471 - val_loss: 0.2501 - val_acc: 0.9162\n",
      "Epoch 40/50\n",
      "7352/7352 [==============================] - 7s 994us/step - loss: 0.1651 - acc: 0.9419 - val_loss: 0.2571 - val_acc: 0.9172\n",
      "Epoch 41/50\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.1696 - acc: 0.9306 - val_loss: 0.2773 - val_acc: 0.9019\n",
      "Epoch 42/50\n",
      "7352/7352 [==============================] - 7s 1ms/step - loss: 0.1732 - acc: 0.9332 - val_loss: 0.2783 - val_acc: 0.9118\n",
      "Epoch 43/50\n",
      "7352/7352 [==============================] - 7s 996us/step - loss: 0.1454 - acc: 0.9455 - val_loss: 0.3407 - val_acc: 0.8741\n",
      "Epoch 44/50\n",
      "7352/7352 [==============================] - 7s 987us/step - loss: 0.1752 - acc: 0.9348 - val_loss: 0.2142 - val_acc: 0.9175\n",
      "Epoch 45/50\n",
      "7352/7352 [==============================] - 7s 995us/step - loss: 0.1398 - acc: 0.9400 - val_loss: 0.2184 - val_acc: 0.9135\n",
      "Epoch 46/50\n",
      "7352/7352 [==============================] - 7s 992us/step - loss: 0.1482 - acc: 0.9434 - val_loss: 0.3776 - val_acc: 0.8694\n",
      "Epoch 47/50\n",
      "7352/7352 [==============================] - 7s 994us/step - loss: 0.1768 - acc: 0.9374 - val_loss: 0.2763 - val_acc: 0.9141\n",
      "Epoch 48/50\n",
      "7352/7352 [==============================] - 7s 990us/step - loss: 0.1255 - acc: 0.9521 - val_loss: 0.2392 - val_acc: 0.9186\n",
      "Epoch 49/50\n",
      "7352/7352 [==============================] - 7s 998us/step - loss: 0.1650 - acc: 0.9321 - val_loss: 0.4025 - val_acc: 0.8931\n",
      "Epoch 50/50\n",
      "7352/7352 [==============================] - 7s 997us/step - loss: 0.1301 - acc: 0.9468 - val_loss: 0.5266 - val_acc: 0.8636\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 537        0  ...                   0                 0\n",
      "SITTING                  1      374  ...                   0                 0\n",
      "STANDING                 0       60  ...                   0                 0\n",
      "WALKING                  0        0  ...                  12                 2\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 405                 3\n",
      "WALKING_UPSTAIRS         0        0  ...                   2               429\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 6s 2ms/step\n",
      "[Loss,Score]:[0.5266424499410406, 0.8635900916185952]\n"
     ]
    }
   ],
   "source": [
    "model=LSTM_Dropout_Grid(50,150,0.6,200,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "confusion_mat(model,X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nIC-VY9er1LI"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Credit:https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_08_4_bayesian_hyperparameter_opt.ipynb\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# Function to create model, required for KerasClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# X_train,Y_train,X_test, Y_test,timesteps, input_dim\n",
    "def evaluate_network(dropout_rate,neuronPct,batch_size_pct):\n",
    "    X_train, X_test, Y_train, Y_test = load_data()\n",
    "    # create model\n",
    "    neuronCount = int(neuronPct * 200)\n",
    "    batch_size=int(batch_size_pct*200)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neuronCount,return_sequences=True, input_shape=(128, 9)))\n",
    "    model.add(LSTM(neuronCount, input_shape=(128, 9)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(6, activation='sigmoid',kernel_initializer=keras.initializers.glorot_normal(seed=0)))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=20)\n",
    "    score = model.evaluate(X_test, Y_test)\n",
    "    return -(score[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8AxEeDmjBNCA"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "OW0slhY6kAqh",
    "outputId": "759c361d-46a7-44bf-c771-a525b50e28a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... | dropou... | neuronPct |\n",
      "-------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 21:29:04.778495 140485343278976 nn_ops.py:4224] Large dropout rate: 0.648292 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 1.7957 - acc: 0.1299 - val_loss: 1.7914 - val_acc: 0.0570\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.7877 - acc: 0.1378 - val_loss: 1.7856 - val_acc: 0.1493\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.7783 - acc: 0.1503 - val_loss: 1.7755 - val_acc: 0.1605\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.7644 - acc: 0.2189 - val_loss: 1.7604 - val_acc: 0.3156\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 40s 5ms/step - loss: 1.7498 - acc: 0.2946 - val_loss: 1.7430 - val_acc: 0.3305\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.7357 - acc: 0.2879 - val_loss: 1.7223 - val_acc: 0.3095\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.7107 - acc: 0.2718 - val_loss: 1.6959 - val_acc: 0.3017\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6849 - acc: 0.2697 - val_loss: 1.6796 - val_acc: 0.3006\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6794 - acc: 0.2708 - val_loss: 1.6618 - val_acc: 0.3078\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6615 - acc: 0.2768 - val_loss: 1.6561 - val_acc: 0.3122\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 40s 5ms/step - loss: 1.6557 - acc: 0.2695 - val_loss: 1.6273 - val_acc: 0.3142\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6409 - acc: 0.2748 - val_loss: 1.6112 - val_acc: 0.3152\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6317 - acc: 0.2831 - val_loss: 1.5919 - val_acc: 0.3196\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6248 - acc: 0.2703 - val_loss: 1.5769 - val_acc: 0.3220\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6132 - acc: 0.2790 - val_loss: 1.5620 - val_acc: 0.3258\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.6077 - acc: 0.2741 - val_loss: 1.5495 - val_acc: 0.3291\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.5925 - acc: 0.2829 - val_loss: 1.5341 - val_acc: 0.3397\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.5792 - acc: 0.2784 - val_loss: 1.5137 - val_acc: 0.3356\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.5616 - acc: 0.2845 - val_loss: 1.4915 - val_acc: 0.3397\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.5532 - acc: 0.2866 - val_loss: 1.4775 - val_acc: 0.3393\n",
      "2947/2947 [==============================] - 17s 6ms/step\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-1.477   \u001b[0m | \u001b[0m 0.4753  \u001b[0m | \u001b[0m 0.6483  \u001b[0m | \u001b[0m 0.01011 \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 56s 8ms/step - loss: 1.4867 - acc: 0.4399 - val_loss: 1.3219 - val_acc: 0.4723\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 1.2057 - acc: 0.4891 - val_loss: 1.1814 - val_acc: 0.4761\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 1.0653 - acc: 0.5484 - val_loss: 1.0452 - val_acc: 0.5507\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.9158 - acc: 0.6466 - val_loss: 0.9560 - val_acc: 0.6491\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.8185 - acc: 0.6859 - val_loss: 0.8570 - val_acc: 0.6390\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.7370 - acc: 0.7121 - val_loss: 0.7524 - val_acc: 0.6905\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.6709 - acc: 0.7308 - val_loss: 0.7006 - val_acc: 0.7078\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.6343 - acc: 0.7456 - val_loss: 0.6499 - val_acc: 0.7289\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.5660 - acc: 0.7621 - val_loss: 0.6885 - val_acc: 0.7190\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.5465 - acc: 0.7682 - val_loss: 0.5897 - val_acc: 0.7452\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.5184 - acc: 0.7724 - val_loss: 0.5594 - val_acc: 0.7503\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.4961 - acc: 0.7771 - val_loss: 0.5351 - val_acc: 0.7553\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 51s 7ms/step - loss: 0.4748 - acc: 0.7901 - val_loss: 0.5552 - val_acc: 0.7560\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.4697 - acc: 0.7877 - val_loss: 0.7317 - val_acc: 0.7075\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.4311 - acc: 0.8098 - val_loss: 0.5637 - val_acc: 0.7533\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.4100 - acc: 0.8225 - val_loss: 0.5117 - val_acc: 0.7828\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3954 - acc: 0.8409 - val_loss: 0.5109 - val_acc: 0.8202\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3566 - acc: 0.8614 - val_loss: 0.5452 - val_acc: 0.8195\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3496 - acc: 0.8745 - val_loss: 0.4561 - val_acc: 0.8463\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 50s 7ms/step - loss: 0.3199 - acc: 0.8840 - val_loss: 0.4756 - val_acc: 0.8398\n",
      "2947/2947 [==============================] - 16s 6ms/step\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.4756  \u001b[0m | \u001b[95m 0.3721  \u001b[0m | \u001b[95m 0.1321  \u001b[0m | \u001b[95m 0.1014  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 75s 10ms/step - loss: 1.1630 - acc: 0.4928 - val_loss: 1.0323 - val_acc: 0.5460\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.7759 - acc: 0.6646 - val_loss: 0.8227 - val_acc: 0.6359\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.6029 - acc: 0.7308 - val_loss: 1.0968 - val_acc: 0.6010\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.5498 - acc: 0.7486 - val_loss: 0.5997 - val_acc: 0.7150\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.4745 - acc: 0.7716 - val_loss: 0.5017 - val_acc: 0.7598\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.4118 - acc: 0.8146 - val_loss: 0.4623 - val_acc: 0.8358\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.3300 - acc: 0.8796 - val_loss: 0.4136 - val_acc: 0.8772\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.2407 - acc: 0.9140 - val_loss: 0.4286 - val_acc: 0.8772\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.1910 - acc: 0.9312 - val_loss: 0.6918 - val_acc: 0.8188\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1736 - acc: 0.9380 - val_loss: 0.4754 - val_acc: 0.8646\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1682 - acc: 0.9412 - val_loss: 0.4443 - val_acc: 0.8873\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.1637 - acc: 0.9392 - val_loss: 0.3608 - val_acc: 0.9087\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.1429 - acc: 0.9460 - val_loss: 0.5390 - val_acc: 0.8785\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1415 - acc: 0.9464 - val_loss: 0.4646 - val_acc: 0.8918\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 69s 9ms/step - loss: 0.1367 - acc: 0.9487 - val_loss: 0.3853 - val_acc: 0.8951\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 70s 9ms/step - loss: 0.1453 - acc: 0.9490 - val_loss: 0.4537 - val_acc: 0.8999\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.1387 - acc: 0.9475 - val_loss: 0.4025 - val_acc: 0.9050\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.1312 - acc: 0.9487 - val_loss: 0.5238 - val_acc: 0.8901\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.1579 - acc: 0.9446 - val_loss: 0.5002 - val_acc: 0.8551\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 70s 10ms/step - loss: 0.1336 - acc: 0.9489 - val_loss: 0.4502 - val_acc: 0.8945\n",
      "2947/2947 [==============================] - 16s 6ms/step\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m-0.4502  \u001b[0m | \u001b[95m 0.2676  \u001b[0m | \u001b[95m 0.311   \u001b[0m | \u001b[95m 0.4028  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 39s 5ms/step - loss: 1.2853 - acc: 0.4471 - val_loss: 1.1471 - val_acc: 0.4985\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.9500 - acc: 0.5766 - val_loss: 0.9610 - val_acc: 0.6043\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.7890 - acc: 0.6465 - val_loss: 0.8145 - val_acc: 0.6926\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.7117 - acc: 0.7046 - val_loss: 0.7616 - val_acc: 0.6854\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.6045 - acc: 0.7640 - val_loss: 0.6660 - val_acc: 0.7645\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.5508 - acc: 0.7828 - val_loss: 0.7588 - val_acc: 0.7279\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.4545 - acc: 0.8303 - val_loss: 0.9726 - val_acc: 0.6050\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.3924 - acc: 0.8613 - val_loss: 0.5865 - val_acc: 0.8127\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.2756 - acc: 0.9057 - val_loss: 0.5846 - val_acc: 0.8222\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 33s 5ms/step - loss: 0.2235 - acc: 0.9174 - val_loss: 0.4575 - val_acc: 0.8731\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.2023 - acc: 0.9261 - val_loss: 0.3988 - val_acc: 0.8588\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1749 - acc: 0.9334 - val_loss: 0.3562 - val_acc: 0.8873\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1583 - acc: 0.9433 - val_loss: 0.3756 - val_acc: 0.8683\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1484 - acc: 0.9419 - val_loss: 0.3813 - val_acc: 0.8850\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 33s 4ms/step - loss: 0.1527 - acc: 0.9407 - val_loss: 0.3136 - val_acc: 0.9077\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1323 - acc: 0.9453 - val_loss: 0.3140 - val_acc: 0.9016\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1457 - acc: 0.9468 - val_loss: 0.3626 - val_acc: 0.8907\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1332 - acc: 0.9484 - val_loss: 0.3671 - val_acc: 0.8975\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1393 - acc: 0.9452 - val_loss: 0.3875 - val_acc: 0.9104\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 0.1353 - acc: 0.9493 - val_loss: 0.3802 - val_acc: 0.9165\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m-0.3802  \u001b[0m | \u001b[95m 0.5849  \u001b[0m | \u001b[95m 0.3773  \u001b[0m | \u001b[95m 0.6884  \u001b[0m |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0829 22:34:49.366941 140485343278976 nn_ops.py:4224] Large dropout rate: 0.790306 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 71s 10ms/step - loss: 1.7196 - acc: 0.2408 - val_loss: 1.6401 - val_acc: 0.4055\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 1.6105 - acc: 0.2969 - val_loss: 1.5524 - val_acc: 0.3261\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 64s 9ms/step - loss: 1.5544 - acc: 0.3009 - val_loss: 1.4723 - val_acc: 0.3353\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.5081 - acc: 0.3150 - val_loss: 1.4548 - val_acc: 0.3495\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.4822 - acc: 0.3066 - val_loss: 1.4126 - val_acc: 0.3186\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.4491 - acc: 0.3035 - val_loss: 1.3867 - val_acc: 0.3013\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.4218 - acc: 0.3181 - val_loss: 1.3522 - val_acc: 0.3498\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.3936 - acc: 0.3380 - val_loss: 1.3300 - val_acc: 0.3638\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.3803 - acc: 0.3339 - val_loss: 1.3206 - val_acc: 0.3777\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.3673 - acc: 0.3396 - val_loss: 1.2993 - val_acc: 0.4126\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.3458 - acc: 0.3640 - val_loss: 1.2883 - val_acc: 0.4588\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.3353 - acc: 0.3692 - val_loss: 1.2653 - val_acc: 0.4903\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.3134 - acc: 0.3785 - val_loss: 1.2431 - val_acc: 0.5025\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.3073 - acc: 0.3679 - val_loss: 1.2321 - val_acc: 0.5025\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.2940 - acc: 0.3840 - val_loss: 1.2281 - val_acc: 0.5215\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.2794 - acc: 0.3958 - val_loss: 1.2175 - val_acc: 0.5202\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.2763 - acc: 0.3847 - val_loss: 1.2082 - val_acc: 0.5266\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.2814 - acc: 0.3785 - val_loss: 1.2052 - val_acc: 0.5212\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.2668 - acc: 0.3879 - val_loss: 1.2009 - val_acc: 0.5114\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 65s 9ms/step - loss: 1.2601 - acc: 0.3853 - val_loss: 1.2134 - val_acc: 0.5019\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-1.213   \u001b[0m | \u001b[0m 0.284   \u001b[0m | \u001b[0m 0.7903  \u001b[0m | \u001b[0m 0.03711 \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 34s 5ms/step - loss: 1.2877 - acc: 0.4472 - val_loss: 1.1777 - val_acc: 0.4815\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.9653 - acc: 0.5858 - val_loss: 1.0530 - val_acc: 0.5660\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.8020 - acc: 0.6492 - val_loss: 0.7909 - val_acc: 0.6963\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.6750 - acc: 0.7137 - val_loss: 0.6798 - val_acc: 0.7234\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 26s 4ms/step - loss: 0.6794 - acc: 0.7365 - val_loss: 0.7054 - val_acc: 0.7201\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.5485 - acc: 0.7878 - val_loss: 0.6056 - val_acc: 0.7808\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.5171 - acc: 0.8098 - val_loss: 0.6878 - val_acc: 0.7296\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.4203 - acc: 0.8507 - val_loss: 0.5536 - val_acc: 0.8147\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.3858 - acc: 0.8613 - val_loss: 0.5097 - val_acc: 0.8181\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.3291 - acc: 0.8851 - val_loss: 0.4164 - val_acc: 0.8565\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.2632 - acc: 0.8959 - val_loss: 0.3730 - val_acc: 0.8700\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.2162 - acc: 0.9252 - val_loss: 0.3472 - val_acc: 0.8897\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1698 - acc: 0.9372 - val_loss: 0.3343 - val_acc: 0.8870\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1948 - acc: 0.9295 - val_loss: 0.2924 - val_acc: 0.8951\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1439 - acc: 0.9456 - val_loss: 0.3603 - val_acc: 0.8979\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1601 - acc: 0.9391 - val_loss: 0.3911 - val_acc: 0.9050\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1490 - acc: 0.9442 - val_loss: 0.4332 - val_acc: 0.8609\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 26s 4ms/step - loss: 0.1656 - acc: 0.9408 - val_loss: 0.4450 - val_acc: 0.8985\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1343 - acc: 0.9478 - val_loss: 0.4353 - val_acc: 0.8890\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 0.1369 - acc: 0.9440 - val_loss: 0.3879 - val_acc: 0.9002\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-0.3879  \u001b[0m | \u001b[0m 0.7034  \u001b[0m | \u001b[0m 0.3756  \u001b[0m | \u001b[0m 0.5631  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 88s 12ms/step - loss: 1.0643 - acc: 0.5411 - val_loss: 0.8901 - val_acc: 0.5898\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.7046 - acc: 0.6726 - val_loss: 0.8402 - val_acc: 0.6702\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.5686 - acc: 0.7605 - val_loss: 0.6644 - val_acc: 0.7421\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.3668 - acc: 0.8613 - val_loss: 0.4532 - val_acc: 0.8334\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.2294 - acc: 0.9188 - val_loss: 0.3418 - val_acc: 0.8989\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1777 - acc: 0.9338 - val_loss: 0.7008 - val_acc: 0.8168\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.1736 - acc: 0.9343 - val_loss: 0.3654 - val_acc: 0.8945\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1554 - acc: 0.9404 - val_loss: 0.3782 - val_acc: 0.8921\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1498 - acc: 0.9427 - val_loss: 0.3545 - val_acc: 0.8989\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1405 - acc: 0.9430 - val_loss: 0.3910 - val_acc: 0.8965\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1503 - acc: 0.9441 - val_loss: 0.4045 - val_acc: 0.9009\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1313 - acc: 0.9498 - val_loss: 0.8113 - val_acc: 0.8324\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 80s 11ms/step - loss: 0.1334 - acc: 0.9520 - val_loss: 0.2504 - val_acc: 0.9135\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1247 - acc: 0.9528 - val_loss: 0.3586 - val_acc: 0.8951\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1260 - acc: 0.9495 - val_loss: 0.3717 - val_acc: 0.9050\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.1199 - acc: 0.9510 - val_loss: 0.3033 - val_acc: 0.9199\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 82s 11ms/step - loss: 0.1203 - acc: 0.9504 - val_loss: 0.3776 - val_acc: 0.9131\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1161 - acc: 0.9542 - val_loss: 0.3267 - val_acc: 0.9091\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1202 - acc: 0.9538 - val_loss: 0.3732 - val_acc: 0.8982\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 81s 11ms/step - loss: 0.1089 - acc: 0.9529 - val_loss: 0.3298 - val_acc: 0.9111\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m-0.3298  \u001b[0m | \u001b[95m 0.2263  \u001b[0m | \u001b[95m 0.1783  \u001b[0m | \u001b[95m 0.8027  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 28s 4ms/step - loss: 1.3236 - acc: 0.4257 - val_loss: 1.1983 - val_acc: 0.4941\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 1.0580 - acc: 0.5427 - val_loss: 1.0961 - val_acc: 0.5487\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.8223 - acc: 0.6421 - val_loss: 0.8358 - val_acc: 0.6488\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.7172 - acc: 0.6806 - val_loss: 0.7318 - val_acc: 0.6732\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.6525 - acc: 0.7116 - val_loss: 0.6668 - val_acc: 0.7112\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.5693 - acc: 0.7519 - val_loss: 0.6377 - val_acc: 0.7065\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.5309 - acc: 0.7644 - val_loss: 0.5545 - val_acc: 0.7594\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.4885 - acc: 0.7874 - val_loss: 0.6875 - val_acc: 0.7289\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.4902 - acc: 0.7912 - val_loss: 0.4920 - val_acc: 0.8076\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.4144 - acc: 0.8426 - val_loss: 0.5215 - val_acc: 0.7933\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.3465 - acc: 0.8783 - val_loss: 0.6379 - val_acc: 0.7849\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.2912 - acc: 0.8979 - val_loss: 0.4580 - val_acc: 0.8409\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.2206 - acc: 0.9217 - val_loss: 0.3541 - val_acc: 0.8785\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.1952 - acc: 0.9308 - val_loss: 0.3423 - val_acc: 0.8870\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.1802 - acc: 0.9325 - val_loss: 0.2797 - val_acc: 0.9030\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.1676 - acc: 0.9368 - val_loss: 0.5317 - val_acc: 0.8395\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.1393 - acc: 0.9460 - val_loss: 0.3710 - val_acc: 0.8809\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.1654 - acc: 0.9434 - val_loss: 0.2727 - val_acc: 0.8989\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.1485 - acc: 0.9456 - val_loss: 0.2931 - val_acc: 0.9087\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 20s 3ms/step - loss: 0.1426 - acc: 0.9463 - val_loss: 0.4916 - val_acc: 0.8249\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m-0.4916  \u001b[0m | \u001b[0m 0.9714  \u001b[0m | \u001b[0m 0.2821  \u001b[0m | \u001b[0m 0.6954  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 1.6751 - acc: 0.2801 - val_loss: 1.5415 - val_acc: 0.4849\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.5209 - acc: 0.3570 - val_loss: 1.4474 - val_acc: 0.5270\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.4536 - acc: 0.3795 - val_loss: 1.3839 - val_acc: 0.5375\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.4014 - acc: 0.4161 - val_loss: 1.3250 - val_acc: 0.5422\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.3532 - acc: 0.4323 - val_loss: 1.3017 - val_acc: 0.5545\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.3015 - acc: 0.4629 - val_loss: 1.2078 - val_acc: 0.5830\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.2606 - acc: 0.4626 - val_loss: 1.1689 - val_acc: 0.5881\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.2287 - acc: 0.4853 - val_loss: 1.1308 - val_acc: 0.6200\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.1854 - acc: 0.4908 - val_loss: 1.0882 - val_acc: 0.5989\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.1769 - acc: 0.4905 - val_loss: 1.0591 - val_acc: 0.5640\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.1402 - acc: 0.4913 - val_loss: 1.0316 - val_acc: 0.5908\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.1113 - acc: 0.4997 - val_loss: 0.9994 - val_acc: 0.5972\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0929 - acc: 0.5030 - val_loss: 0.9855 - val_acc: 0.5931\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0750 - acc: 0.5178 - val_loss: 0.9515 - val_acc: 0.6067\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0575 - acc: 0.5126 - val_loss: 0.9355 - val_acc: 0.6105\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0589 - acc: 0.5122 - val_loss: 0.9572 - val_acc: 0.6145\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0618 - acc: 0.5029 - val_loss: 0.9114 - val_acc: 0.6030\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0442 - acc: 0.5098 - val_loss: 0.9105 - val_acc: 0.6088\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0222 - acc: 0.5139 - val_loss: 0.8959 - val_acc: 0.6010\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0110 - acc: 0.5209 - val_loss: 0.8844 - val_acc: 0.6121\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m-0.8844  \u001b[0m | \u001b[0m 0.8888  \u001b[0m | \u001b[0m 0.8051  \u001b[0m | \u001b[0m 0.09419 \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 145s 20ms/step - loss: 1.0195 - acc: 0.5541 - val_loss: 0.8705 - val_acc: 0.5918\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 138s 19ms/step - loss: 0.7490 - acc: 0.6851 - val_loss: 0.7842 - val_acc: 0.7011\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.5163 - acc: 0.8055 - val_loss: 0.4626 - val_acc: 0.8239\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 136s 18ms/step - loss: 0.2648 - acc: 0.9057 - val_loss: 0.3901 - val_acc: 0.8914\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 0.1935 - acc: 0.9306 - val_loss: 0.7123 - val_acc: 0.8405\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 136s 19ms/step - loss: 0.1615 - acc: 0.9363 - val_loss: 0.2926 - val_acc: 0.9053\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 0.1526 - acc: 0.9445 - val_loss: 0.3009 - val_acc: 0.9019\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 136s 18ms/step - loss: 0.1417 - acc: 0.9430 - val_loss: 0.2924 - val_acc: 0.9118\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1421 - acc: 0.9442 - val_loss: 0.3292 - val_acc: 0.9033\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1383 - acc: 0.9486 - val_loss: 0.3022 - val_acc: 0.9104\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 0.1445 - acc: 0.9459 - val_loss: 0.3464 - val_acc: 0.9138\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1348 - acc: 0.9501 - val_loss: 0.2799 - val_acc: 0.9155\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 0.1271 - acc: 0.9479 - val_loss: 0.2990 - val_acc: 0.9108\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 0.1275 - acc: 0.9484 - val_loss: 0.6776 - val_acc: 0.8619\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 137s 19ms/step - loss: 0.1205 - acc: 0.9529 - val_loss: 0.2977 - val_acc: 0.9023\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 136s 18ms/step - loss: 0.1266 - acc: 0.9514 - val_loss: 0.3551 - val_acc: 0.9162\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1091 - acc: 0.9538 - val_loss: 0.2893 - val_acc: 0.9237\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 134s 18ms/step - loss: 0.1151 - acc: 0.9546 - val_loss: 0.3663 - val_acc: 0.9104\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 136s 19ms/step - loss: 0.1299 - acc: 0.9531 - val_loss: 0.3964 - val_acc: 0.9121\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 135s 18ms/step - loss: 0.1187 - acc: 0.9527 - val_loss: 0.3613 - val_acc: 0.9230\n",
      "2947/2947 [==============================] - 16s 6ms/step\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m-0.3613  \u001b[0m | \u001b[0m 0.1351  \u001b[0m | \u001b[0m 0.1528  \u001b[0m | \u001b[0m 0.8794  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 192s 26ms/step - loss: 1.1922 - acc: 0.5058 - val_loss: 0.9299 - val_acc: 0.5494\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.9835 - acc: 0.5902 - val_loss: 0.7912 - val_acc: 0.5830\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.9592 - acc: 0.5880 - val_loss: 0.7963 - val_acc: 0.6274\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 185s 25ms/step - loss: 0.8007 - acc: 0.6249 - val_loss: 0.8223 - val_acc: 0.6105\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 185s 25ms/step - loss: 0.7463 - acc: 0.6375 - val_loss: 0.7350 - val_acc: 0.6264\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 0.7089 - acc: 0.6440 - val_loss: 0.6890 - val_acc: 0.6508\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.6913 - acc: 0.6576 - val_loss: 0.7318 - val_acc: 0.6135\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.5946 - acc: 0.7305 - val_loss: 0.5224 - val_acc: 0.8059\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.5584 - acc: 0.8077 - val_loss: 0.5380 - val_acc: 0.7927\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.4770 - acc: 0.8670 - val_loss: 0.5359 - val_acc: 0.8629\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 180s 24ms/step - loss: 0.3856 - acc: 0.8924 - val_loss: 0.5343 - val_acc: 0.8544\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 0.4670 - acc: 0.8400 - val_loss: 0.5738 - val_acc: 0.8687\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 180s 25ms/step - loss: 0.2845 - acc: 0.9138 - val_loss: 0.5120 - val_acc: 0.8931\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 1.7669 - acc: 0.1778 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 180s 24ms/step - loss: 1.7918 - acc: 0.1668 - val_loss: 1.7912 - val_acc: 0.1683\n",
      "2947/2947 [==============================] - 17s 6ms/step\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m-1.791   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 27s 4ms/step - loss: 1.7801 - acc: 0.1749 - val_loss: 1.7745 - val_acc: 0.1673\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.7643 - acc: 0.2764 - val_loss: 1.7584 - val_acc: 0.3095\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.7475 - acc: 0.3213 - val_loss: 1.7434 - val_acc: 0.2942\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.7302 - acc: 0.3115 - val_loss: 1.7275 - val_acc: 0.2973\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.7134 - acc: 0.3515 - val_loss: 1.7142 - val_acc: 0.4075\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.6971 - acc: 0.4740 - val_loss: 1.6991 - val_acc: 0.4533\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.6788 - acc: 0.4852 - val_loss: 1.6839 - val_acc: 0.4527\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.6610 - acc: 0.4833 - val_loss: 1.6749 - val_acc: 0.4401\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.6459 - acc: 0.4771 - val_loss: 1.6603 - val_acc: 0.4384\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.6263 - acc: 0.4833 - val_loss: 1.6415 - val_acc: 0.4442\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.6061 - acc: 0.4857 - val_loss: 1.6273 - val_acc: 0.4411\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5868 - acc: 0.4856 - val_loss: 1.6103 - val_acc: 0.4428\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5711 - acc: 0.4823 - val_loss: 1.6042 - val_acc: 0.4293\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5628 - acc: 0.4668 - val_loss: 1.6101 - val_acc: 0.3987\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5698 - acc: 0.4306 - val_loss: 1.6205 - val_acc: 0.3648\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5808 - acc: 0.3904 - val_loss: 1.6284 - val_acc: 0.3312\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5781 - acc: 0.3696 - val_loss: 1.6271 - val_acc: 0.3098\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5671 - acc: 0.3595 - val_loss: 1.6129 - val_acc: 0.3108\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5528 - acc: 0.3530 - val_loss: 1.6034 - val_acc: 0.3051\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 18s 2ms/step - loss: 1.5371 - acc: 0.3459 - val_loss: 1.5940 - val_acc: 0.3044\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-1.594   \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 1.4338 - acc: 0.3972 - val_loss: 1.3004 - val_acc: 0.3556\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.2002 - acc: 0.4918 - val_loss: 1.1579 - val_acc: 0.5589\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0210 - acc: 0.5680 - val_loss: 1.0318 - val_acc: 0.5439\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.9329 - acc: 0.5842 - val_loss: 0.8843 - val_acc: 0.5874\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.8759 - acc: 0.6024 - val_loss: 1.0450 - val_acc: 0.5691\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.8034 - acc: 0.6163 - val_loss: 0.8102 - val_acc: 0.6050\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.7724 - acc: 0.6226 - val_loss: 0.8346 - val_acc: 0.6332\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.7165 - acc: 0.6394 - val_loss: 0.7881 - val_acc: 0.6617\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.7251 - acc: 0.6375 - val_loss: 0.7048 - val_acc: 0.6152\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6890 - acc: 0.6396 - val_loss: 0.7339 - val_acc: 0.6318\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.7726 - acc: 0.6310 - val_loss: 0.9856 - val_acc: 0.6366\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6663 - acc: 0.6517 - val_loss: 1.3621 - val_acc: 0.5752\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.7126 - acc: 0.6469 - val_loss: 0.8475 - val_acc: 0.6478\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6896 - acc: 0.6465 - val_loss: 0.8205 - val_acc: 0.6098\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6699 - acc: 0.6484 - val_loss: 1.0277 - val_acc: 0.5999\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6808 - acc: 0.6393 - val_loss: 0.7160 - val_acc: 0.6220\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6684 - acc: 0.6453 - val_loss: 0.7805 - val_acc: 0.6240\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6320 - acc: 0.6619 - val_loss: 0.8641 - val_acc: 0.6403\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6729 - acc: 0.6556 - val_loss: 0.6743 - val_acc: 0.6349\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6517 - acc: 0.6549 - val_loss: 0.8209 - val_acc: 0.6573\n",
      "2947/2947 [==============================] - 16s 6ms/step\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m-0.8209  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 191s 26ms/step - loss: 1.0253 - acc: 0.5673 - val_loss: 0.7996 - val_acc: 0.6980\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.6548 - acc: 0.7216 - val_loss: 0.6165 - val_acc: 0.7241\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.5103 - acc: 0.7693 - val_loss: 0.5646 - val_acc: 0.7547\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.3649 - acc: 0.8512 - val_loss: 0.3789 - val_acc: 0.8673\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 180s 25ms/step - loss: 0.1952 - acc: 0.9290 - val_loss: 0.3935 - val_acc: 0.8880\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 179s 24ms/step - loss: 0.1635 - acc: 0.9404 - val_loss: 0.3112 - val_acc: 0.9033\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 179s 24ms/step - loss: 0.1433 - acc: 0.9453 - val_loss: 0.2600 - val_acc: 0.9121\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.1408 - acc: 0.9456 - val_loss: 0.2706 - val_acc: 0.9074\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 178s 24ms/step - loss: 0.1472 - acc: 0.9464 - val_loss: 0.2993 - val_acc: 0.9172\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 178s 24ms/step - loss: 0.1339 - acc: 0.9504 - val_loss: 0.3860 - val_acc: 0.8924\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 178s 24ms/step - loss: 0.1290 - acc: 0.9509 - val_loss: 0.2305 - val_acc: 0.9192\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 180s 24ms/step - loss: 0.1169 - acc: 0.9533 - val_loss: 0.3754 - val_acc: 0.9026\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.1230 - acc: 0.9520 - val_loss: 0.4647 - val_acc: 0.9019\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 180s 25ms/step - loss: 0.1218 - acc: 0.9516 - val_loss: 0.3925 - val_acc: 0.9030\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.1211 - acc: 0.9525 - val_loss: 0.2862 - val_acc: 0.9250\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.1299 - acc: 0.9512 - val_loss: 0.3864 - val_acc: 0.9040\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 180s 25ms/step - loss: 0.1222 - acc: 0.9528 - val_loss: 0.4168 - val_acc: 0.9158\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 179s 24ms/step - loss: 0.1220 - acc: 0.9490 - val_loss: 0.3841 - val_acc: 0.9233\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 180s 24ms/step - loss: 0.1184 - acc: 0.9531 - val_loss: 0.3722 - val_acc: 0.9111\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 180s 24ms/step - loss: 0.1187 - acc: 0.9542 - val_loss: 0.2336 - val_acc: 0.9169\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m-0.2336  \u001b[0m | \u001b[95m 0.1     \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m 0.3482  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 40s 5ms/step - loss: 1.2708 - acc: 0.4558 - val_loss: 1.0212 - val_acc: 0.5572\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.8686 - acc: 0.6072 - val_loss: 0.9115 - val_acc: 0.5942\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.7624 - acc: 0.6670 - val_loss: 0.8140 - val_acc: 0.6434\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.6758 - acc: 0.7114 - val_loss: 0.7205 - val_acc: 0.7109\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5876 - acc: 0.7583 - val_loss: 0.8859 - val_acc: 0.6410\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.5035 - acc: 0.7972 - val_loss: 0.6865 - val_acc: 0.7336\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.4122 - acc: 0.8434 - val_loss: 0.5606 - val_acc: 0.8076\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 31s 4ms/step - loss: 0.3383 - acc: 0.8770 - val_loss: 0.7894 - val_acc: 0.7211\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.2938 - acc: 0.8958 - val_loss: 0.5175 - val_acc: 0.8215\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.2427 - acc: 0.9101 - val_loss: 0.4373 - val_acc: 0.8622\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1913 - acc: 0.9278 - val_loss: 0.4086 - val_acc: 0.8850\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1698 - acc: 0.9363 - val_loss: 0.3442 - val_acc: 0.9006\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1686 - acc: 0.9343 - val_loss: 0.3059 - val_acc: 0.9006\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1613 - acc: 0.9343 - val_loss: 0.3097 - val_acc: 0.9026\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1468 - acc: 0.9445 - val_loss: 0.3520 - val_acc: 0.8768\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1669 - acc: 0.9329 - val_loss: 0.3285 - val_acc: 0.9060\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 0.1334 - acc: 0.9483 - val_loss: 0.5806 - val_acc: 0.8504\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1416 - acc: 0.9406 - val_loss: 0.2968 - val_acc: 0.9084\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1447 - acc: 0.9450 - val_loss: 0.5035 - val_acc: 0.8517\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 29s 4ms/step - loss: 0.1631 - acc: 0.9406 - val_loss: 0.3093 - val_acc: 0.9155\n",
      "2947/2947 [==============================] - 16s 5ms/step\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.3093  \u001b[0m | \u001b[0m 0.6545  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 45s 6ms/step - loss: 1.2685 - acc: 0.4603 - val_loss: 1.1691 - val_acc: 0.4839\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.9077 - acc: 0.5993 - val_loss: 0.9442 - val_acc: 0.6176\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.7111 - acc: 0.7035 - val_loss: 0.9438 - val_acc: 0.5870\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.6456 - acc: 0.7304 - val_loss: 0.8846 - val_acc: 0.6383\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.5553 - acc: 0.7666 - val_loss: 0.7482 - val_acc: 0.7194\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.4826 - acc: 0.8200 - val_loss: 0.8567 - val_acc: 0.7122\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.4417 - acc: 0.8419 - val_loss: 0.5847 - val_acc: 0.7760\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.3452 - acc: 0.8751 - val_loss: 0.5166 - val_acc: 0.8381\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.2807 - acc: 0.9037 - val_loss: 0.6652 - val_acc: 0.7648\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.2398 - acc: 0.9176 - val_loss: 0.4506 - val_acc: 0.8649\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.2010 - acc: 0.9305 - val_loss: 0.3770 - val_acc: 0.8887\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1650 - acc: 0.9359 - val_loss: 0.3091 - val_acc: 0.8924\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1603 - acc: 0.9387 - val_loss: 0.3528 - val_acc: 0.8962\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 43s 6ms/step - loss: 0.1660 - acc: 0.9374 - val_loss: 0.4070 - val_acc: 0.8510\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1589 - acc: 0.9358 - val_loss: 0.3844 - val_acc: 0.8914\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1466 - acc: 0.9455 - val_loss: 0.3876 - val_acc: 0.8785\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1328 - acc: 0.9444 - val_loss: 0.3691 - val_acc: 0.8823\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1212 - acc: 0.9532 - val_loss: 0.3487 - val_acc: 0.8897\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1609 - acc: 0.9373 - val_loss: 0.3208 - val_acc: 0.8999\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1385 - acc: 0.9426 - val_loss: 0.2709 - val_acc: 0.9162\n",
      "2947/2947 [==============================] - 16s 6ms/step\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m-0.2709  \u001b[0m | \u001b[0m 0.5106  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.5371  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 192s 26ms/step - loss: 1.7585 - acc: 0.2348 - val_loss: 1.7010 - val_acc: 0.3346\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.6425 - acc: 0.3511 - val_loss: 1.5980 - val_acc: 0.3356\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.5437 - acc: 0.3957 - val_loss: 1.5126 - val_acc: 0.3627\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.4723 - acc: 0.3798 - val_loss: 1.4540 - val_acc: 0.3624\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.4273 - acc: 0.3799 - val_loss: 1.4246 - val_acc: 0.3665\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 1.3861 - acc: 0.3765 - val_loss: 1.3900 - val_acc: 0.3583\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.3423 - acc: 0.3675 - val_loss: 1.3644 - val_acc: 0.3427\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.3115 - acc: 0.3667 - val_loss: 1.3496 - val_acc: 0.3420\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 1.2876 - acc: 0.3802 - val_loss: 1.3264 - val_acc: 0.3468\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.2651 - acc: 0.3891 - val_loss: 1.3047 - val_acc: 0.3655\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 1.2357 - acc: 0.4127 - val_loss: 1.2818 - val_acc: 0.4001\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 1.2013 - acc: 0.4499 - val_loss: 1.2728 - val_acc: 0.4276\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.1579 - acc: 0.5109 - val_loss: 1.2410 - val_acc: 0.5029\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.1078 - acc: 0.5507 - val_loss: 1.2061 - val_acc: 0.5097\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 1.0601 - acc: 0.5505 - val_loss: 1.1721 - val_acc: 0.5226\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 1.0303 - acc: 0.5577 - val_loss: 1.1507 - val_acc: 0.5209\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 1.0108 - acc: 0.5653 - val_loss: 1.1400 - val_acc: 0.5436\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.9899 - acc: 0.5781 - val_loss: 1.0866 - val_acc: 0.5643\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 0.9648 - acc: 0.5892 - val_loss: 1.1850 - val_acc: 0.5266\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.9438 - acc: 0.5899 - val_loss: 1.0547 - val_acc: 0.5650\n",
      "2947/2947 [==============================] - 16s 6ms/step\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m-1.055   \u001b[0m | \u001b[0m 0.1     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.01    \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 30s 4ms/step - loss: 1.4710 - acc: 0.3727 - val_loss: 1.2448 - val_acc: 0.4903\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 1.2818 - acc: 0.4626 - val_loss: 1.1435 - val_acc: 0.5612\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 1.1422 - acc: 0.5298 - val_loss: 1.1430 - val_acc: 0.5158\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 1.0370 - acc: 0.5502 - val_loss: 0.9146 - val_acc: 0.5748\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.9209 - acc: 0.6051 - val_loss: 0.8491 - val_acc: 0.6420\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.9156 - acc: 0.6054 - val_loss: 0.8058 - val_acc: 0.6043\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.9594 - acc: 0.5947 - val_loss: 0.8629 - val_acc: 0.5796\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.8968 - acc: 0.5993 - val_loss: 0.8771 - val_acc: 0.5779\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.8561 - acc: 0.6171 - val_loss: 0.8140 - val_acc: 0.6057\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7982 - acc: 0.6235 - val_loss: 0.7622 - val_acc: 0.6128\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.8870 - acc: 0.5979 - val_loss: 0.7154 - val_acc: 0.6088\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7756 - acc: 0.6232 - val_loss: 0.7314 - val_acc: 0.6210\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7541 - acc: 0.6390 - val_loss: 0.7401 - val_acc: 0.6250\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7401 - acc: 0.6302 - val_loss: 0.7868 - val_acc: 0.6281\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7313 - acc: 0.6387 - val_loss: 0.7647 - val_acc: 0.6115\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7861 - acc: 0.6239 - val_loss: 0.7476 - val_acc: 0.6166\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7227 - acc: 0.6363 - val_loss: 0.7598 - val_acc: 0.6138\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7391 - acc: 0.6374 - val_loss: 0.8297 - val_acc: 0.6172\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7325 - acc: 0.6344 - val_loss: 0.7581 - val_acc: 0.6261\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 19s 3ms/step - loss: 0.7521 - acc: 0.6386 - val_loss: 0.7633 - val_acc: 0.6240\n",
      "2947/2947 [==============================] - 16s 6ms/step\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m-0.7633  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.9     \u001b[0m | \u001b[0m 0.5126  \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 32s 4ms/step - loss: 1.3976 - acc: 0.4057 - val_loss: 1.2654 - val_acc: 0.4740\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 1.0712 - acc: 0.5393 - val_loss: 0.9743 - val_acc: 0.5836\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.8330 - acc: 0.6162 - val_loss: 1.5855 - val_acc: 0.4340\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.8309 - acc: 0.6245 - val_loss: 0.9528 - val_acc: 0.5534\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.7282 - acc: 0.6717 - val_loss: 0.8550 - val_acc: 0.6376\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.6450 - acc: 0.7282 - val_loss: 0.7014 - val_acc: 0.7228\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.5575 - acc: 0.7724 - val_loss: 0.7079 - val_acc: 0.7156\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.4940 - acc: 0.8073 - val_loss: 0.6882 - val_acc: 0.7665\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.4385 - acc: 0.8313 - val_loss: 0.6403 - val_acc: 0.7740\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.3420 - acc: 0.8694 - val_loss: 0.6231 - val_acc: 0.7930\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.2981 - acc: 0.8886 - val_loss: 0.4015 - val_acc: 0.8690\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.2250 - acc: 0.9157 - val_loss: 0.3792 - val_acc: 0.8711\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.2203 - acc: 0.9212 - val_loss: 1.0459 - val_acc: 0.7455\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.2023 - acc: 0.9297 - val_loss: 0.4952 - val_acc: 0.8398\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.1652 - acc: 0.9328 - val_loss: 0.3222 - val_acc: 0.8914\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.1500 - acc: 0.9453 - val_loss: 0.3781 - val_acc: 0.8850\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.1580 - acc: 0.9415 - val_loss: 0.3571 - val_acc: 0.8897\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.1474 - acc: 0.9438 - val_loss: 0.3181 - val_acc: 0.9033\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.1431 - acc: 0.9457 - val_loss: 0.3281 - val_acc: 0.9019\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 21s 3ms/step - loss: 0.1250 - acc: 0.9505 - val_loss: 0.3566 - val_acc: 0.8979\n",
      "2947/2947 [==============================] - 17s 6ms/step\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m-0.3566  \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m |\n",
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 196s 27ms/step - loss: 1.0023 - acc: 0.5503 - val_loss: 0.7064 - val_acc: 0.6973\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 185s 25ms/step - loss: 0.4823 - acc: 0.8078 - val_loss: 0.4137 - val_acc: 0.8636\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.2619 - acc: 0.9104 - val_loss: 0.4740 - val_acc: 0.8442\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 185s 25ms/step - loss: 0.1854 - acc: 0.9294 - val_loss: 0.3381 - val_acc: 0.8870\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.1636 - acc: 0.9388 - val_loss: 0.2463 - val_acc: 0.9043\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.1453 - acc: 0.9433 - val_loss: 0.3172 - val_acc: 0.9043\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 181s 25ms/step - loss: 0.1356 - acc: 0.9483 - val_loss: 0.2644 - val_acc: 0.9141\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.1388 - acc: 0.9472 - val_loss: 0.3657 - val_acc: 0.8979\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 182s 25ms/step - loss: 0.1412 - acc: 0.9495 - val_loss: 0.5018 - val_acc: 0.8741\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 0.1321 - acc: 0.9529 - val_loss: 0.2976 - val_acc: 0.9182\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 0.1202 - acc: 0.9521 - val_loss: 0.4057 - val_acc: 0.8951\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 183s 25ms/step - loss: 0.1300 - acc: 0.9497 - val_loss: 0.3263 - val_acc: 0.9097\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.1193 - acc: 0.9523 - val_loss: 0.4177 - val_acc: 0.9002\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 191s 26ms/step - loss: 0.1212 - acc: 0.9521 - val_loss: 0.2973 - val_acc: 0.9233\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.1170 - acc: 0.9528 - val_loss: 0.3019 - val_acc: 0.9162\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 184s 25ms/step - loss: 0.1125 - acc: 0.9558 - val_loss: 0.3150 - val_acc: 0.9125\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 185s 25ms/step - loss: 0.1196 - acc: 0.9514 - val_loss: 0.3399 - val_acc: 0.9125\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 185s 25ms/step - loss: 0.1305 - acc: 0.9527 - val_loss: 0.3438 - val_acc: 0.9118\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 185s 25ms/step - loss: 0.1236 - acc: 0.9543 - val_loss: 0.3239 - val_acc: 0.9060\n",
      "Epoch 20/20\n",
      "6820/7352 [==========================>...] - ETA: 11s - loss: 0.1155 - acc: 0.9566"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "import time\n",
    "\n",
    "# Supress NaN warnings, see: https://stackoverflow.com/questions/34955158/what-might-be-the-cause-of-invalid-value-encountered-in-less-equal-in-numpy\n",
    "# Bounded region of parameter space epoch_pct,dropout_rate,neuronPct,batch_size_pct\n",
    "pbounds = {'neuronPct': (0.01, 1),\n",
    "           'batch_size_pct':(0.1,1),\n",
    "           'dropout_rate': (0.0, 0.9),\n",
    "          }\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=evaluate_network,\n",
    "    pbounds=pbounds,\n",
    "    verbose=2,  # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "start_time = time.time()\n",
    "optimizer.maximize(init_points=10, n_iter=20,)\n",
    "time_took = time.time() - start_time\n",
    "\n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrcdPBxR-8ed"
   },
   "source": [
    "## 2 Layer LSTM With and with out Drop Out Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ny1zYrqhFuWg"
   },
   "outputs": [],
   "source": [
    "def twoLSTM_Dropout_(epochs,n_hidden,D,batch_S,X_train,Y_train,X_test, Y_test,timesteps, input_dim):\n",
    "    # Initiliazing the sequential model\n",
    "    n_classes=6\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_hidden,return_sequences=True, input_shape=(128, 9)))\n",
    "    model.add(LSTM(n_hidden, input_shape=(128, 9)))\n",
    "    model.add(Dropout(D))\n",
    "    model.add(Dense(6, activation='sigmoid',kernel_initializer=keras.initializers.glorot_normal(seed=0)))\n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    # Training the model\n",
    "    model.fit(X_train,\n",
    "              Y_train,\n",
    "              batch_size=batch_S,\n",
    "              validation_data=(X_test, Y_test),\n",
    "              epochs=epochs)\n",
    "    return model\n",
    "\n",
    "def confusion_mat(model,X_test,Y_test):\n",
    "  print(confusion_matrix(Y_test, model.predict(X_test)))\n",
    "  score = model.evaluate(X_test, Y_test)\n",
    "  print(\"[Loss,Score]:{}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 918
    },
    "colab_type": "code",
    "id": "-OP-yjAhEHSB",
    "outputId": "7f85059c-26b8-43a7-b4ad-f2bc7f0db2f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7352 samples, validate on 2947 samples\n",
      "Epoch 1/20\n",
      "7352/7352 [==============================] - 38s 5ms/step - loss: 1.2478 - acc: 0.4652 - val_loss: 1.1066 - val_acc: 0.5141\n",
      "Epoch 2/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.8331 - acc: 0.6198 - val_loss: 0.8862 - val_acc: 0.6210\n",
      "Epoch 3/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.7087 - acc: 0.6889 - val_loss: 0.8677 - val_acc: 0.6664\n",
      "Epoch 4/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.6166 - acc: 0.7379 - val_loss: 0.8709 - val_acc: 0.6688\n",
      "Epoch 5/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.5305 - acc: 0.7851 - val_loss: 0.6064 - val_acc: 0.7801\n",
      "Epoch 6/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.4972 - acc: 0.8096 - val_loss: 0.5457 - val_acc: 0.8188\n",
      "Epoch 7/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.3775 - acc: 0.8629 - val_loss: 0.5773 - val_acc: 0.8039\n",
      "Epoch 8/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.3016 - acc: 0.8911 - val_loss: 0.3316 - val_acc: 0.8755\n",
      "Epoch 9/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.2313 - acc: 0.9149 - val_loss: 0.3710 - val_acc: 0.8782\n",
      "Epoch 10/20\n",
      "7352/7352 [==============================] - 37s 5ms/step - loss: 0.1900 - acc: 0.9301 - val_loss: 0.4069 - val_acc: 0.8744\n",
      "Epoch 11/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1754 - acc: 0.9358 - val_loss: 0.3764 - val_acc: 0.8951\n",
      "Epoch 12/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1596 - acc: 0.9429 - val_loss: 0.3725 - val_acc: 0.8833\n",
      "Epoch 13/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1299 - acc: 0.9479 - val_loss: 0.3612 - val_acc: 0.8999\n",
      "Epoch 14/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1445 - acc: 0.9449 - val_loss: 0.3223 - val_acc: 0.9002\n",
      "Epoch 15/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1761 - acc: 0.9392 - val_loss: 0.3456 - val_acc: 0.8999\n",
      "Epoch 16/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1256 - acc: 0.9510 - val_loss: 0.4013 - val_acc: 0.8432\n",
      "Epoch 17/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1422 - acc: 0.9455 - val_loss: 0.3604 - val_acc: 0.8979\n",
      "Epoch 18/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1296 - acc: 0.9478 - val_loss: 0.3055 - val_acc: 0.9114\n",
      "Epoch 19/20\n",
      "7352/7352 [==============================] - 36s 5ms/step - loss: 0.1212 - acc: 0.9508 - val_loss: 0.3344 - val_acc: 0.9036\n",
      "Epoch 20/20\n",
      "7352/7352 [==============================] - 35s 5ms/step - loss: 0.1238 - acc: 0.9501 - val_loss: 0.3887 - val_acc: 0.8999\n",
      "Pred                LAYING  SITTING  ...  WALKING_DOWNSTAIRS  WALKING_UPSTAIRS\n",
      "True                                 ...                                      \n",
      "LAYING                 510        0  ...                   0                27\n",
      "SITTING                  5      377  ...                   0                 2\n",
      "STANDING                 0       84  ...                   0                 0\n",
      "WALKING                  0        0  ...                  11                25\n",
      "WALKING_DOWNSTAIRS       0        0  ...                 419                 1\n",
      "WALKING_UPSTAIRS         0        0  ...                  11               441\n",
      "\n",
      "[6 rows x 6 columns]\n",
      "2947/2947 [==============================] - 18s 6ms/step\n",
      "[Loss,Score]:[0.3886889021135067, 0.8998982015609094]\n"
     ]
    }
   ],
   "source": [
    "#epochs,n_hidden,D,batch_S\n",
    "X_train, X_test, Y_train, Y_test = load_data()\n",
    "model=twoLSTM_Dropout_(20,149,0.075,109,X_train,Y_train,X_test, Y_test,timesteps, input_dim)\n",
    "confusion_mat(model,X_test,Y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GmKL4_QZ1Pa6"
   },
   "source": [
    "## Conclusion:\n",
    "\n",
    "* Best Accuracy observed by trying various modifications to Hyperparameters like (Hidden Layers,Dropout Rate and 2 Layer LSTM with High Dropout Rate is Shown below\n",
    "* Top Accuracy Till now is with 2 layer LSTM with 128 Hidden layer and no Dropout train accuracy is 0.9524 and test accuracy is 0.9270 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BlPU3jWx6Qid"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>N_hidden Layers</th>\n",
    "    <th>Drop Out Rate</th> \n",
    "    <th>Train Accuracy</th>\n",
    "    <th>Train Loss</th>\n",
    "    <th>Test Accuracy</th>\n",
    "    <th>Test Loss</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>64</td>\n",
    "    <td>0.5</td> \n",
    "    <td>0.9429</td>\n",
    "    <td>0.1695</td>\n",
    "    <td>0.8900</td>\n",
    "    <td>0.495</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>100</td>\n",
    "    <td>0.5</td>\n",
    "    <td>0.9465</td>\n",
    "    <td>0.1407</td>\n",
    "    <td>0.9056</td>\n",
    "    <td>0.3020</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>0.5</td>\n",
    "    <td>0.9497</td>\n",
    "    <td>0.1308</td>\n",
    "    <td>0.91211</td>\n",
    "    <td>0.4315</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>150</td>\n",
    "    <td>0.5</td>\n",
    "    <td>0.9467</td>\n",
    "    <td>0.1462</td>\n",
    "    <td>0.8985</td>\n",
    "    <td>0.5622</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>0.6</td>\n",
    "    <td>0.9475</td>\n",
    "    <td>0.1416</td>\n",
    "    <td>0.9100</td>\n",
    "    <td>0.3308</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>0.7</td>\n",
    "    <td>0.9444</td>\n",
    "    <td>0.1645</td>\n",
    "    <td>0.9205</td>\n",
    "    <td>0.4161</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>0.8</td>\n",
    "    <td>0.9287</td>\n",
    "    <td>0.2544</td>\n",
    "    <td>0.888</td>\n",
    "    <td>0.566</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>0.4</td>\n",
    "    <td>0.9459</td>\n",
    "    <td>0.1343</td>\n",
    "    <td>0.887</td>\n",
    "    <td>0.5019</td>\n",
    "  </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbnUv3jm-9zw"
   },
   "source": [
    "<table style=\"width:100%\">\n",
    "  <tr>\n",
    "    <th>N_hidden Layers</th>\n",
    "    <th>Drop Out Rate</th> \n",
    "    <th>Train Accuracy</th>\n",
    "    <th>Train Loss</th>\n",
    "    <th>Test Accuracy</th>\n",
    "    <th>Test Loss</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>NONE</td>\n",
    "    <td>0.9524</td>\n",
    "    <td>0.1181</td>\n",
    "    <td>0.927</td>\n",
    "    <td>0.3255</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>0.7</td>\n",
    "    <td>0.9498</td>\n",
    "    <td>0.1489</td>\n",
    "    <td>0.8951</td>\n",
    "    <td>0.3848</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>128</td>\n",
    "    <td>0.8</td>\n",
    "    <td>0.9411</td>\n",
    "    <td>0.1657</td>\n",
    "    <td>0.8859</td>\n",
    "    <td>0.5410</td>\n",
    "  </tr>\n",
    "  </table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIOBDz3saCia"
   },
   "source": [
    "# HAR Using Divide and Conquer\n",
    "## Credit: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5949027/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "colab_type": "code",
    "id": "m4myvk7iIyMc",
    "outputId": "5829c0f9-8e0d-4054-8d4a-e1931d670725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mafj-c4DgBIV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sQY2LWZcgdV3"
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DATADIR = f'/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/HAR/UCI_HAR_Dataset'\n",
    "# Raw data signals\n",
    "# Signals are from Accelerometer and Gyroscope\n",
    "# The signals are in x,y,z directions\n",
    "# Sensor signals are filtered to have only body acceleration\n",
    "# excluding the acceleration due to gravity\n",
    "# Triaxial acceleration from the accelerometer is total acceleration\n",
    "SIGNALS = [\n",
    "    \"body_acc_x\",\n",
    "    \"body_acc_y\",\n",
    "    \"body_acc_z\",\n",
    "    \"body_gyro_x\",\n",
    "    \"body_gyro_y\",\n",
    "    \"body_gyro_z\",\n",
    "    \"total_acc_x\",\n",
    "    \"total_acc_y\",\n",
    "    \"total_acc_z\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAHiU4-ik5oq"
   },
   "outputs": [],
   "source": [
    "# Utility function to read the data from csv file\n",
    "def _read_csv(filename):\n",
    "    return pd.read_csv(filename, delim_whitespace=True, header=None)\n",
    "\n",
    "# Utility function to load the load\n",
    "def load_signals(subset):\n",
    "    signals_data = []\n",
    "\n",
    "    for signal in SIGNALS:\n",
    "        filename = f'/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/HAR/UCI_HAR_Dataset/{subset}/Inertial Signals/{signal}_{subset}.txt'\n",
    "        signals_data.append(_read_csv(filename).values\n",
    "        ) \n",
    "\n",
    "    # Transpose is used to change the dimensionality of the output,\n",
    "    # aggregating the signals by combination of sample/timestep.\n",
    "    # Resultant shape is (7352 train/2947 test samples, 128 timesteps, 9 signals)\n",
    "    return np.transpose(signals_data, (1, 2, 0))\n",
    "\n",
    "def load_y(subset):\n",
    "    \"\"\"\n",
    "    The objective that we are trying to predict is a integer, from 1 to 6,\n",
    "    that represents a human activity. We return a binary representation of \n",
    "    every sample objective as a 6 bits vector using One Hot Encoding\n",
    "    (https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html)\n",
    "    \"\"\"\n",
    "    filename = f'/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/HAR/UCI_HAR_Dataset/{subset}/y_{subset}.txt'\n",
    "    y = _read_csv(filename)[0]\n",
    "    return y\n",
    "    \n",
    "def load_data():\n",
    "    \"\"\"\n",
    "    Obtain the dataset from multiple files.\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X_train, X_test = load_signals('train'), load_signals('test')\n",
    "    y_train, y_test = load_y('train'), load_y('test')\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "P9pRnQ28l_ol",
    "outputId": "8e9a9d5e-dd97-4821-cc8a-58044fbc6b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "9\n",
      "7352\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Utility function to count the number of classes\n",
    "def _count_classes(y):\n",
    "    return len(set([tuple(category) for category in y]))\n",
    "# Loading the train and test data\n",
    "x_train,x_test, y_train, y_test = load_data()\n",
    "timesteps = len(x_train[0])\n",
    "input_dim = len(x_train[0][0])\n",
    "# n_classes = _count_classes(y_train)\n",
    "\n",
    "print(timesteps)\n",
    "print(input_dim)\n",
    "# print(n_classes)\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTPh1UaChoS6"
   },
   "source": [
    "## Spliting Labels into Dynamic and Static"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d2qEaP4wmh5N"
   },
   "outputs": [],
   "source": [
    "y_train_clf=pd.get_dummies([0 if i<=3 else 1 for i in y_train]).values\n",
    "y_test_clf=pd.get_dummies([0 if i<=3 else 1 for i in y_test]).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0p_PBGronKq"
   },
   "source": [
    "## Model For Classifying Data Into Static and Dynamic Activities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QewTKZosojvy"
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,BatchNormalization\n",
    "from keras.layers import Conv1D,MaxPooling1D,Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 403
    },
    "colab_type": "code",
    "id": "UYZQmyeBpIro",
    "outputId": "da7cb5d1-b196-4d52-a7a2-a1a34a972790"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "460/460 [==============================] - 2s 4ms/step - loss: 0.0396 - accuracy: 0.9841 - val_loss: 0.0228 - val_accuracy: 0.9946\n",
      "Epoch 2/10\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 0.0025 - accuracy: 0.9990 - val_loss: 0.0263 - val_accuracy: 0.9939\n",
      "Epoch 3/10\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 0.0023 - accuracy: 0.9996 - val_loss: 0.0647 - val_accuracy: 0.9800\n",
      "Epoch 4/10\n",
      "460/460 [==============================] - 2s 3ms/step - loss: 4.3877e-04 - accuracy: 0.9999 - val_loss: 0.0101 - val_accuracy: 0.9973\n",
      "Epoch 5/10\n",
      "460/460 [==============================] - 2s 3ms/step - loss: 6.7860e-05 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9966\n",
      "Epoch 6/10\n",
      "460/460 [==============================] - 2s 3ms/step - loss: 1.5671e-05 - accuracy: 1.0000 - val_loss: 0.0132 - val_accuracy: 0.9969\n",
      "Epoch 7/10\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 6.3452e-06 - accuracy: 1.0000 - val_loss: 0.0135 - val_accuracy: 0.9969\n",
      "Epoch 8/10\n",
      "460/460 [==============================] - 2s 3ms/step - loss: 9.2745e-06 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 0.9973\n",
      "Epoch 9/10\n",
      "460/460 [==============================] - 2s 3ms/step - loss: 4.0150e-06 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 0.9973\n",
      "Epoch 10/10\n",
      "460/460 [==============================] - 1s 3ms/step - loss: 9.5808e-06 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 0.9963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fdd5c1b8d68>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, activation='relu',kernel_initializer='he_uniform'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train,y_train_clf, epochs=10, batch_size=16,validation_data=(x_test, y_test_clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PSAYWM6RqKAq"
   },
   "outputs": [],
   "source": [
    "model.save('/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/model_st_dy_clf.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P2y2eYGTtQBa"
   },
   "source": [
    "## Static Activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IQh9r0sqtjh"
   },
   "outputs": [],
   "source": [
    "static_indices_train=np.where(y_train>3)[0].tolist()\n",
    "static_indices_test=np.where(y_test>3)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mehuXDAxrt6h"
   },
   "outputs": [],
   "source": [
    "x_train_st=x_train[static_indices_train]\n",
    "x_test_st=x_test[static_indices_test]\n",
    "y_train_st=pd.get_dummies(y_train[static_indices_train]).values\n",
    "y_test_st=pd.get_dummies(y_test[static_indices_test]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "BV1ah3akssy-",
    "outputId": "4c49e496-71af-4b84-fe1c-95adf91c7aaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4067, 3)\n",
      "(1560, 3)\n",
      "(1560, 128, 9)\n",
      "(4067, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_st.shape)\n",
    "print(y_test_st.shape)\n",
    "print(x_test_st.shape)\n",
    "print(x_train_st.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "805YQqEqtZdg"
   },
   "source": [
    "## Model Static Activities Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zkVWy8WCb7Sg"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import Callback\n",
    "\n",
    "class TerminateOnBaseline(Callback):\n",
    "    \"\"\"Callback that terminates training when either acc or val_acc reaches a specified baseline\n",
    "    \"\"\"\n",
    "    def __init__(self, monitor='acc', baseline=0.9):\n",
    "        super(TerminateOnBaseline, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.baseline = baseline\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        acc = logs.get(self.monitor)\n",
    "        if acc is not None:\n",
    "            if acc >= self.baseline:\n",
    "                print('Epoch %d: Reached baseline, terminating training' % (epoch))\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "2yvjYfaJCmzI",
    "outputId": "b19d6791-faab-4244-ca61-c19439228545"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.3722 - accuracy: 0.8539 - val_loss: 0.3284 - val_accuracy: 0.8808\n",
      "Epoch 2/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2657 - accuracy: 0.9004 - val_loss: 0.3884 - val_accuracy: 0.8833\n",
      "Epoch 3/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.9007 - val_loss: 0.3316 - val_accuracy: 0.8865\n",
      "Epoch 4/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9088 - val_loss: 0.3337 - val_accuracy: 0.8763\n",
      "Epoch 5/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2298 - accuracy: 0.9169 - val_loss: 0.3322 - val_accuracy: 0.8737\n",
      "Epoch 6/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2280 - accuracy: 0.9103 - val_loss: 0.3337 - val_accuracy: 0.8776\n",
      "Epoch 7/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2264 - accuracy: 0.9159 - val_loss: 0.4840 - val_accuracy: 0.8423\n",
      "Epoch 8/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1937 - accuracy: 0.9243 - val_loss: 0.3487 - val_accuracy: 0.8910\n",
      "Epoch 9/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2039 - accuracy: 0.9211 - val_loss: 0.3271 - val_accuracy: 0.8776\n",
      "Epoch 10/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1863 - accuracy: 0.9213 - val_loss: 0.3076 - val_accuracy: 0.8872\n",
      "Epoch 11/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1841 - accuracy: 0.9265 - val_loss: 0.3777 - val_accuracy: 0.8654\n",
      "Epoch 12/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1900 - accuracy: 0.9282 - val_loss: 0.2989 - val_accuracy: 0.8994\n",
      "Epoch 13/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.2034 - accuracy: 0.9299 - val_loss: 0.3321 - val_accuracy: 0.8821\n",
      "Epoch 14/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1736 - accuracy: 0.9302 - val_loss: 0.2811 - val_accuracy: 0.9045\n",
      "Epoch 15/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1804 - accuracy: 0.9319 - val_loss: 0.3438 - val_accuracy: 0.8865\n",
      "Epoch 16/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1690 - accuracy: 0.9309 - val_loss: 0.2573 - val_accuracy: 0.9090\n",
      "Epoch 17/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1701 - accuracy: 0.9319 - val_loss: 0.2967 - val_accuracy: 0.8942\n",
      "Epoch 18/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1743 - accuracy: 0.9331 - val_loss: 0.4997 - val_accuracy: 0.8603\n",
      "Epoch 19/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1608 - accuracy: 0.9383 - val_loss: 0.2985 - val_accuracy: 0.8981\n",
      "Epoch 20/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1455 - accuracy: 0.9427 - val_loss: 0.2671 - val_accuracy: 0.9058\n",
      "Epoch 21/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1479 - accuracy: 0.9422 - val_loss: 0.3731 - val_accuracy: 0.9032\n",
      "Epoch 22/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1470 - accuracy: 0.9412 - val_loss: 0.2775 - val_accuracy: 0.9160\n",
      "Epoch 23/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1469 - accuracy: 0.9427 - val_loss: 0.2701 - val_accuracy: 0.9244\n",
      "Epoch 24/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1484 - accuracy: 0.9407 - val_loss: 0.4489 - val_accuracy: 0.8917\n",
      "Epoch 25/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1617 - accuracy: 0.9462 - val_loss: 0.3017 - val_accuracy: 0.9115\n",
      "Epoch 26/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1398 - accuracy: 0.9466 - val_loss: 0.3700 - val_accuracy: 0.8763\n",
      "Epoch 27/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1608 - accuracy: 0.9452 - val_loss: 0.2943 - val_accuracy: 0.9147\n",
      "Epoch 28/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1486 - accuracy: 0.9513 - val_loss: 0.5666 - val_accuracy: 0.8603\n",
      "Epoch 29/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1361 - accuracy: 0.9503 - val_loss: 0.3111 - val_accuracy: 0.8942\n",
      "Epoch 30/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1475 - accuracy: 0.9496 - val_loss: 0.3367 - val_accuracy: 0.8936\n",
      "Epoch 31/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1491 - accuracy: 0.9434 - val_loss: 0.3082 - val_accuracy: 0.9154\n",
      "Epoch 32/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1267 - accuracy: 0.9523 - val_loss: 0.3895 - val_accuracy: 0.9103\n",
      "Epoch 33/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1147 - accuracy: 0.9607 - val_loss: 0.5296 - val_accuracy: 0.8981\n",
      "Epoch 34/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1165 - accuracy: 0.9543 - val_loss: 0.5362 - val_accuracy: 0.8917\n",
      "Epoch 35/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1285 - accuracy: 0.9562 - val_loss: 0.5486 - val_accuracy: 0.8756\n",
      "Epoch 36/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1281 - accuracy: 0.9557 - val_loss: 0.4324 - val_accuracy: 0.8846\n",
      "Epoch 37/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1094 - accuracy: 0.9602 - val_loss: 0.2806 - val_accuracy: 0.9109\n",
      "Epoch 38/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1314 - accuracy: 0.9575 - val_loss: 0.2853 - val_accuracy: 0.9212\n",
      "Epoch 39/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1109 - accuracy: 0.9607 - val_loss: 0.4576 - val_accuracy: 0.8647\n",
      "Epoch 40/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1075 - accuracy: 0.9614 - val_loss: 0.2965 - val_accuracy: 0.9192\n",
      "Epoch 41/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1107 - accuracy: 0.9621 - val_loss: 0.4390 - val_accuracy: 0.8974\n",
      "Epoch 42/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1162 - accuracy: 0.9599 - val_loss: 0.3274 - val_accuracy: 0.9199\n",
      "Epoch 43/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1138 - accuracy: 0.9597 - val_loss: 0.3548 - val_accuracy: 0.8987\n",
      "Epoch 44/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0965 - accuracy: 0.9666 - val_loss: 0.3996 - val_accuracy: 0.8936\n",
      "Epoch 45/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1025 - accuracy: 0.9626 - val_loss: 0.3446 - val_accuracy: 0.9083\n",
      "Epoch 46/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0984 - accuracy: 0.9648 - val_loss: 0.2334 - val_accuracy: 0.9314\n",
      "Epoch 47/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1026 - accuracy: 0.9636 - val_loss: 0.3457 - val_accuracy: 0.9115\n",
      "Epoch 48/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1129 - accuracy: 0.9639 - val_loss: 0.3049 - val_accuracy: 0.9192\n",
      "Epoch 49/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0935 - accuracy: 0.9666 - val_loss: 0.5120 - val_accuracy: 0.9103\n",
      "Epoch 50/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0966 - accuracy: 0.9678 - val_loss: 0.5189 - val_accuracy: 0.8821\n",
      "Epoch 51/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0941 - accuracy: 0.9663 - val_loss: 0.4581 - val_accuracy: 0.9141\n",
      "Epoch 52/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1178 - accuracy: 0.9643 - val_loss: 0.2899 - val_accuracy: 0.9263\n",
      "Epoch 53/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0873 - accuracy: 0.9680 - val_loss: 0.3507 - val_accuracy: 0.9083\n",
      "Epoch 54/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0997 - accuracy: 0.9661 - val_loss: 0.4464 - val_accuracy: 0.9109\n",
      "Epoch 55/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0861 - accuracy: 0.9727 - val_loss: 0.5104 - val_accuracy: 0.9058\n",
      "Epoch 56/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0998 - accuracy: 0.9680 - val_loss: 0.4404 - val_accuracy: 0.8936\n",
      "Epoch 57/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0953 - accuracy: 0.9675 - val_loss: 0.3460 - val_accuracy: 0.9237\n",
      "Epoch 58/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0855 - accuracy: 0.9673 - val_loss: 0.5605 - val_accuracy: 0.8929\n",
      "Epoch 59/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0943 - accuracy: 0.9653 - val_loss: 0.7833 - val_accuracy: 0.8833\n",
      "Epoch 60/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.1153 - accuracy: 0.9661 - val_loss: 0.5047 - val_accuracy: 0.9109\n",
      "Epoch 61/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0948 - accuracy: 0.9702 - val_loss: 0.6546 - val_accuracy: 0.8853\n",
      "Epoch 62/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0884 - accuracy: 0.9710 - val_loss: 0.3930 - val_accuracy: 0.9051\n",
      "Epoch 63/5000\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0929 - accuracy: 0.9678 - val_loss: 0.4079 - val_accuracy: 0.8994\n",
      "Epoch 64/5000\n",
      "255/255 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9698Epoch 63: Reached baseline, terminating training\n",
      "255/255 [==============================] - 1s 5ms/step - loss: 0.0830 - accuracy: 0.9698 - val_loss: 0.2949 - val_accuracy: 0.9404\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f10ce5e8588>"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "callbacks = [TerminateOnBaseline(monitor='val_accuracy', baseline=0.94)]\n",
    "model_st_1=Sequential()\n",
    "model_st_1.add(Conv1D(filters=64, kernel_size=5,padding='same', activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model_st_1.add(Conv1D(filters=32, kernel_size=3,padding='same', activation='relu',kernel_initializer='he_uniform'))\n",
    "model_st_1.add(Dropout(0.4))\n",
    "model_st_1.add(MaxPooling1D(pool_size=1,strides=1))\n",
    "model_st_1.add(Flatten())\n",
    "model_st_1.add(Dense(64, activation='relu',kernel_initializer='he_uniform'))\n",
    "model_st_1.add(BatchNormalization()) \n",
    "model_st_1.add(Dropout(0.4))\n",
    "model_st_1.add(Dense(32, activation='relu',kernel_initializer='he_uniform'))\n",
    "model_st_1.add(BatchNormalization()) \n",
    "model_st_1.add(Dropout(0.4))\n",
    "model_st_1.add(Dense(3, activation='softmax'))\n",
    "model_st_1.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model_st_1.fit(x_train_st,y_train_st, epochs=5000, batch_size=16,validation_data=(x_test_st, y_test_st),callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sgWkA3r1uk-R"
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "# Utility function to print the confusion matrix\n",
    "def confusion_matrix_cnn(Y_true, Y_pred,activities):\n",
    "    Y_true = pd.Series([activities[y] for y in np.argmax(Y_true, axis=1)])\n",
    "    Y_pred = pd.Series([activities[y] for y in np.argmax(Y_pred, axis=1)])\n",
    "\n",
    "    #return pd.crosstab(Y_true, Y_pred, rownames=['True'], colnames=['Pred'])\n",
    "    return confusion_matrix(Y_true, Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "colab_type": "code",
    "id": "EmRoufwoz3Qr",
    "outputId": "7f257ad1-6fdb-48e0-ae0c-c36ca319dba4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[537   0   0]\n",
      " [  0 422  69]\n",
      " [  0  24 508]]\n",
      "49/49 [==============================] - 0s 3ms/step - loss: 0.2949 - accuracy: 0.9404\n",
      "[0.29486751556396484, 0.9403846263885498]\n"
     ]
    }
   ],
   "source": [
    "activites_stat = {\n",
    "    0: 'SITTING',\n",
    "    1: 'STANDING',\n",
    "    2: 'LAYING',\n",
    "}\n",
    "\n",
    "# Confusion Matrix\n",
    "print(confusion_matrix_cnn(y_test_st, model_st_1.predict(x_test_st),activites_stat))\n",
    "print(model_st_1.evaluate(x_test_st,y_test_st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "WP3xx5ld0BQt",
    "outputId": "fd0c05ca-2b45-4c87-c464-6fd434b8f992"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "cm = confusion_matrix_cnn(y_test_st, model_st_1.predict(x_test_st),activites_stat)\n",
    "plot_confusion_matrix(cm, classes=['SITTING','STANDING','LAYING'], normalize=True, title='Normalized confusion matrix', cmap = plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1svjqf8r0J7m"
   },
   "outputs": [],
   "source": [
    "model_st_1.save('/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/model_static_class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQZ6JA2HGSHa"
   },
   "source": [
    "## Dynamic Classes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qBzLVhjVGafz"
   },
   "outputs": [],
   "source": [
    "dynamic_indices_train=np.where(y_train<=3)[0].tolist()\n",
    "dynamic_indices_test=np.where(y_test<=3)[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B6xtD60L0fLp"
   },
   "outputs": [],
   "source": [
    "x_train_dy=x_train[dynamic_indices_train]\n",
    "x_test_dy=x_test[dynamic_indices_test]\n",
    "y_train_dy=pd.get_dummies(y_train[dynamic_indices_train]).values\n",
    "y_test_dy=pd.get_dummies(y_test[dynamic_indices_test]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "Vk4WDqG3G2Ow",
    "outputId": "8a89f627-1e7c-4dbc-f9f1-d880e5e21ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3285, 3)\n",
      "(1387, 3)\n",
      "(1387, 128, 9)\n",
      "(3285, 128, 9)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_dy.shape)\n",
    "print(y_test_dy.shape)\n",
    "print(x_test_dy.shape)\n",
    "print(x_train_dy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "07ZeY6QIG8yQ",
    "outputId": "8be433b5-2136-4961-db45-9ecb42e6a1f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.3498 - accuracy: 0.8734 - val_loss: 0.7135 - val_accuracy: 0.9553\n",
      "Epoch 2/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0351 - accuracy: 0.9921 - val_loss: 0.2379 - val_accuracy: 0.9589\n",
      "Epoch 3/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0304 - accuracy: 0.9921 - val_loss: 0.2304 - val_accuracy: 0.9373\n",
      "Epoch 4/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0099 - accuracy: 0.9985 - val_loss: 0.1064 - val_accuracy: 0.9676\n",
      "Epoch 5/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0028 - accuracy: 0.9997 - val_loss: 0.1246 - val_accuracy: 0.9654\n",
      "Epoch 6/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.1036 - val_accuracy: 0.9683\n",
      "Epoch 7/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 9.5750e-04 - accuracy: 1.0000 - val_loss: 0.1699 - val_accuracy: 0.9618\n",
      "Epoch 8/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0410 - accuracy: 0.9866 - val_loss: 0.0928 - val_accuracy: 0.9719\n",
      "Epoch 9/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.0170 - accuracy: 0.9948 - val_loss: 0.1263 - val_accuracy: 0.9640\n",
      "Epoch 10/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9647\n",
      "Epoch 11/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 9.0676e-04 - accuracy: 1.0000 - val_loss: 0.1106 - val_accuracy: 0.9697\n",
      "Epoch 12/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.1419 - val_accuracy: 0.9661\n",
      "Epoch 13/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 8.9139e-04 - accuracy: 1.0000 - val_loss: 0.0762 - val_accuracy: 0.9805\n",
      "Epoch 14/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 4.6589e-04 - accuracy: 1.0000 - val_loss: 0.1298 - val_accuracy: 0.9697\n",
      "Epoch 15/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.0249 - accuracy: 0.9918 - val_loss: 0.1916 - val_accuracy: 0.9589\n",
      "Epoch 16/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0073 - accuracy: 0.9985 - val_loss: 0.1622 - val_accuracy: 0.9481\n",
      "Epoch 17/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.1518 - val_accuracy: 0.9697\n",
      "Epoch 18/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9997 - val_loss: 0.2154 - val_accuracy: 0.9503\n",
      "Epoch 19/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0131 - accuracy: 0.9973 - val_loss: 0.1554 - val_accuracy: 0.9611\n",
      "Epoch 20/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0085 - accuracy: 0.9979 - val_loss: 0.0626 - val_accuracy: 0.9769\n",
      "Epoch 21/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 7.4261e-04 - accuracy: 0.9997 - val_loss: 0.0653 - val_accuracy: 0.9805\n",
      "Epoch 22/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 8.4021e-04 - accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9827\n",
      "Epoch 23/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.0444 - val_accuracy: 0.9870\n",
      "Epoch 24/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.0037 - accuracy: 0.9994 - val_loss: 0.5487 - val_accuracy: 0.9077\n",
      "Epoch 25/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0168 - accuracy: 0.9948 - val_loss: 0.0312 - val_accuracy: 0.9899\n",
      "Epoch 26/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 9.4508e-04 - accuracy: 0.9997 - val_loss: 0.0668 - val_accuracy: 0.9805\n",
      "Epoch 27/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.0069 - accuracy: 0.9979 - val_loss: 0.0380 - val_accuracy: 0.9913\n",
      "Epoch 28/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0020 - accuracy: 0.9997 - val_loss: 0.0132 - val_accuracy: 0.9971\n",
      "Epoch 29/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.0082 - accuracy: 0.9976 - val_loss: 0.0443 - val_accuracy: 0.9834\n",
      "Epoch 30/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0190 - accuracy: 0.9954 - val_loss: 0.1398 - val_accuracy: 0.9690\n",
      "Epoch 31/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.4852 - val_accuracy: 0.9164\n",
      "Epoch 32/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0057 - accuracy: 0.9988 - val_loss: 0.2051 - val_accuracy: 0.9582\n",
      "Epoch 33/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 0.0016 - accuracy: 0.9994 - val_loss: 0.0220 - val_accuracy: 0.9921\n",
      "Epoch 34/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 6.7791e-04 - accuracy: 1.0000 - val_loss: 0.0327 - val_accuracy: 0.9921\n",
      "Epoch 35/40\n",
      "206/206 [==============================] - 1s 4ms/step - loss: 4.7151e-04 - accuracy: 1.0000 - val_loss: 0.0474 - val_accuracy: 0.9899\n",
      "Epoch 36/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 2.5275e-04 - accuracy: 1.0000 - val_loss: 0.0494 - val_accuracy: 0.9899\n",
      "Epoch 37/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 1.5584e-04 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 0.9906\n",
      "Epoch 38/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 2.2711e-04 - accuracy: 1.0000 - val_loss: 0.0466 - val_accuracy: 0.9906\n",
      "Epoch 39/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 2.3141e-04 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 0.9921\n",
      "Epoch 40/40\n",
      "206/206 [==============================] - 1s 3ms/step - loss: 9.6326e-05 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 0.9913\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f11ce29cd68>"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dy = Sequential()\n",
    "model_dy.add(Conv1D(64,5,activation='relu',kernel_initializer='he_uniform',input_shape=(128,9)))\n",
    "model_dy.add(Conv1D(32,5,activation='relu',kernel_initializer='he_uniform'))\n",
    "model_dy.add(Flatten())\n",
    "model_dy.add(Dense(36,activation='softmax'))\n",
    "model_dy.add(BatchNormalization())\n",
    "model_dy.add(Dense(3,activation='softmax'))\n",
    "model_dy.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_dy.fit(x_train_dy,y_train_dy, epochs=40, batch_size=16,validation_data=(x_test_dy, y_test_dy),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "MmM4pwqkH0Ea",
    "outputId": "2fe3dadd-e806-4c9c-a1a7-63fc13154775"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44/44 [==============================] - 0s 2ms/step - loss: 0.0355 - accuracy: 0.9913\n"
     ]
    }
   ],
   "source": [
    "score = model_dy.evaluate(x_test_dy, y_test_dy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "lW2LeV-JIHoj",
    "outputId": "d1f086a1-3435-4b98-d925-057d416a9ada"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1226    0    0]\n",
      " [   0  986    0]\n",
      " [   0    0 1073]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import metrics\n",
    "activities_dyn = {\n",
    "    0: 'WALKING',\n",
    "    1: 'WALKING_UPSTAIRS',\n",
    "    2: 'WALKING_DOWNSTAIRS',\n",
    "}\n",
    "\n",
    "\n",
    "# Confusion Matrix\n",
    "print(confusion_matrix_cnn(y_train_dy, model_dy.predict(x_train_dy),activities_dyn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "colab_type": "code",
    "id": "2rYP_FnqIQ8Y",
    "outputId": "83a0d638-b804-4445-9ff7-3422d64fb53c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "cm = confusion_matrix_cnn(y_train_dy, model_dy.predict(x_train_dy),activities_dyn)\n",
    "plot_confusion_matrix(cm, classes=['WALKING','WALKING_UPSTAIRS','WALKING_DOWNSTAIRS'], \n",
    "                      normalize=True, title='Normalized confusion matrix', cmap = plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EX4raiWsIbJD"
   },
   "outputs": [],
   "source": [
    "model_dy.save('/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/model_dyn_class.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JuOxotTuIqq4"
   },
   "source": [
    "## Final Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xELsvtjRIoE4"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pickle\n",
    "model_clf=load_model('/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/model_st_dy_clf.h5')\n",
    "model_st_clf=load_model('/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/model_static_class.h5')\n",
    "model_dy_clf=load_model('/content/drive/My Drive/Projects/LSTM_HUMAN/Human_Activity_Recognition/model_dyn_class.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QaGNB5rhJcBb"
   },
   "outputs": [],
   "source": [
    "pred_clf_st_dy=model_clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TPqHEJiCJlFS"
   },
   "outputs": [],
   "source": [
    "y_pred_clf_st_dy=np.argmax(pred_clf_st_dy,axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W-f0-lY4JxYu"
   },
   "outputs": [],
   "source": [
    "X_static=x_test[y_pred_clf_st_dy ==1]\n",
    "X_dynamic = x_test[y_pred_clf_st_dy ==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xY6lC4WrJzAK"
   },
   "outputs": [],
   "source": [
    "pred_st=model_st_clf.predict(X_static)\n",
    "pred_st=np.argmax(pred_st,axis=1)\n",
    "pred_st=pred_st+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sHx0m0XDKcmm"
   },
   "outputs": [],
   "source": [
    "pred_dy=model_dy_clf.predict(X_dynamic)\n",
    "pred_dy=np.argmax(pred_dy,axis=1)\n",
    "pred_dy=pred_dy+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1TGjRNPNKqA_"
   },
   "outputs": [],
   "source": [
    "#appending final output to one list in the same sequence of input data\n",
    "i,j = 0,0 \n",
    "final_pred = []\n",
    "for mask in y_pred_clf_st_dy:\n",
    "    if mask == 1:\n",
    "        final_pred.append(pred_st[i])\n",
    "        i = i + 1\n",
    "    else:\n",
    "        final_pred.append(pred_dy[j])\n",
    "        j = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xJ5ITMbRLJZe",
    "outputId": "cd3f398a-06fe-4e8e-9b08-700a2d44cb20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of test data 0.9616559212758737\n"
     ]
    }
   ],
   "source": [
    "##accuracy of test\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('Accuracy of test data',accuracy_score(y_test,final_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "785lmUNCiPJz"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "* Acheived Greater than 96% Test Accuracy using Divide and Conquer Method"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Copy of HAR_LSTM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
